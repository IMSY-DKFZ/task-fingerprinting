{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:30.034866Z",
     "start_time": "2025-05-19T10:20:25.462523Z"
    }
   },
   "source": [
    "# this notebook generates all commands for the recent mml version \n",
    "import dataclasses\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Union\n",
    "\n",
    "try:\n",
    "    import mml.interactive\n",
    "except ImportError:\n",
    "    raise RuntimeError('This reproduction expects a recent version of MML - please refer to the README for detailed instructions.')\n",
    "\n",
    "mml.interactive.init(Path('~/.config/mml.env').expanduser())\n",
    "from mml.interactive import DefaultRequirements, MMLJobDescription\n",
    "from mml_tf.tasks import all_tasks, get_valid_sources, shrinkable_tasks, target_tasks, source_tasks, train_tasks, \\\n",
    "    all_tasks_including_shrunk, task_infos\n",
    "\n",
    "CLUSTER_USAGE = False  # change if you (want / do not want) to run on the cluster\n",
    "\n",
    "if CLUSTER_USAGE:\n",
    "    from mml_lsf.requirements import LSFSubmissionRequirements"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _____ ______   _____ ______   ___\n",
      "|\\   _ \\  _   \\|\\   _ \\  _   \\|\\  \\\n",
      "\\ \\  \\\\\\__\\ \\  \\ \\  \\\\\\__\\ \\  \\ \\  \\\n",
      " \\ \\  \\\\|__| \\  \\ \\  \\\\|__| \\  \\ \\  \\\n",
      "  \\ \\  \\    \\ \\  \\ \\  \\    \\ \\  \\ \\  \\____\n",
      "   \\ \\__\\    \\ \\__\\ \\__\\    \\ \\__\\ \\_______\\\n",
      "    \\|__|     \\|__|\\|__|     \\|__|\\|_______|\n",
      "         ____  _  _    __  _  _  ____  _  _\n",
      "        (  _ \\( \\/ )  (  )( \\/ )/ ___)( \\/ )\n",
      "         ) _ ( )  /    )( / \\/ \\\\___ \\ )  /\n",
      "        (____/(__/    (__)\\_)(_/(____/(__/\n",
      "Interactive MML API initialized.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:30.840898Z",
     "start_time": "2025-05-19T10:20:30.838745Z"
    }
   },
   "source": [
    "# note that final experiments have to be run multiple times to ensure significance\n",
    "rerun = 3"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:31.367614Z",
     "start_time": "2025-05-19T10:20:31.362899Z"
    }
   },
   "source": [
    "if CLUSTER_USAGE:\n",
    "    # cluster submission prepends, add yours here in case you have other gpu requirements\n",
    "    base_reqs = LSFSubmissionRequirements(special_requirements=[],\n",
    "                                          undesired_hosts=['e230-dgx2-2', 'e230-dgxa100-2', 'e230-dgxa100-4',\n",
    "                                                           'e230-dgxa100-1',\n",
    "                                                           'e230-dgxa100-2', 'e230-dgxa100-3', 'e230-dgxa100-4',\n",
    "                                                           'lsf22-gpu08', 'lsf22-gpu01', 'lsf22-gpu02', 'lsf22-gpu03',\n",
    "                                                           'lsf22-gpu04', 'lsf22-gpu05', 'lsf22-gpu06', 'lsf22-gpu07'],\n",
    "                                          num_gpus=1, vram_per_gpu=11.0, queue='gpu-lowprio',\n",
    "                                          mail='EMAIL.ADDRESS@dkfz-heidelberg.de', script_name='mml.sh',\n",
    "                                          job_group='/USERNAME/pami_rerun'\n",
    "                                          )\n",
    "    # see for example this local setup\n",
    "    # base_reqs = pp_reqs = aa_reqs = def_reqs = arch_reqs = tl_reqs = multi_reqs = nb.DefaultRequirements()\n",
    "    pp_reqs = dataclasses.replace(base_reqs, queue='gpu')\n",
    "    aa_reqs = dataclasses.replace(base_reqs, script_name='aa.sh', vram_per_gpu=13.0)\n",
    "    def_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'])\n",
    "    tl_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'], vram_per_gpu=24.0)\n",
    "    multi_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'], vram_per_gpu=14.0)\n",
    "else:\n",
    "    base_reqs = pp_reqs = aa_reqs = def_reqs = tl_reqs = multi_reqs = DefaultRequirements()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:31.658813Z",
     "start_time": "2025-05-19T10:20:31.655082Z"
    }
   },
   "source": [
    "# project overview -> points to MML projects we use, we will append indices for each \"rerun\"\n",
    "projects = {\n",
    "    'base': 'pami2_base_02',\n",
    "    'raw_baseline': 'pami2_raw_03',\n",
    "    'raw_shrunk': 'pami2_raw_shrunk_10',\n",
    "    # aa search can not be carried out with recent MML version, we provide the created policies in data/auto_augmentations\n",
    "    'aa_search': 'pami2_t_aa_search_01',\n",
    "    # the above are shared with train (!) since stuff is only computed once anyway\n",
    "    'transfer': 'pami2_t_transfer_20',\n",
    "    'multi_task': 'test_multi_balanced_test_split_10', \n",
    "    'multi_shrunk': 'test_multi_balanced_shrunk_test_split_10',\n",
    "    'arch_search': 'pami2_t_arch_search_02',\n",
    "    'arch_infer': 'pami2_t_arch_infer_02',\n",
    "    'aa_infer': 'pami2_t_aa_infer_02'\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:31.856576Z",
     "start_time": "2025-05-19T10:20:31.850724Z"
    }
   },
   "source": [
    "# prepare steps\n",
    "prep_cmds = list()\n",
    "# step one: plain task creation\n",
    "\n",
    "prep_cmds.append(MMLJobDescription(prefix_req=pp_reqs, mode='create',\n",
    "                                   config_options={'tasks': 'pami', 'proj': projects['base']}))\n",
    "# step two: plain task preprocessing\n",
    "prep_cmds.append(MMLJobDescription(prefix_req=pp_reqs, mode='pp',\n",
    "                                   config_options={'tasks': 'pami', 'proj': projects['base']}))\n",
    "# step three: shrunk preprocessing\n",
    "prep_cmds.append(MMLJobDescription(prefix_req=pp_reqs, mode='info',\n",
    "                                   config_options={'tasks': 'pami_shrinkable_800', 'proj': projects['base']}))\n",
    "mml.interactive.write_out_commands(cmd_list=prep_cmds, name='prepare')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 3 commands at prepare.txt.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:32.046550Z",
     "start_time": "2025-05-19T10:20:32.041953Z"
    }
   },
   "source": [
    "# OPTIONALLY: compute dimensions (used for Fig. 3) and some additional experiments not shown in the paper\n",
    "dim_cmds = list()\n",
    "dim_cmds.append(MMLJobDescription(prefix_req=def_reqs, mode='dim', config_options={'tasks': 'pami_shrink_mix',\n",
    "                                                                                   'proj': projects[\"base\"],\n",
    "                                                                                   'mode.inv_mle': True}))\n",
    "mml.interactive.write_out_commands(cmd_list=dim_cmds, name='dimensions')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 1 commands at dimensions.txt.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:32.228183Z",
     "start_time": "2025-05-19T10:20:32.223508Z"
    }
   },
   "source": [
    "# convenience function for easier retrieve from cluster results, \n",
    "user_id = 'USERNAME'  \n",
    "# use as print(get_retrieve_for_proj('my_project')) and run the result in a shell to get the results of 'my_project' from cluster to your local system\n",
    "def get_retrieve_for_proj(proj):\n",
    "    return f\"rsync -rtvu --stats --exclude=PARAMETERS --exclude=hpo --exclude=runs --exclude=FIMS --exclude=FC_TUNED {user_id}@odcf-worker01:{os.getenv('MML_CLUSTER_RESULTS_PATH')}/{proj}/ {os.getenv('MML_RESULTS_PATH')}/{proj}\"\n",
    "\n",
    "\n",
    "# the following optimizes a jobs epochs in a way that target task is seen at least 40 epochs but at max 4000 steps (plus finishing the epoch)\n",
    "def optimize_epochs(target_task: str, batch_size: int = 300, max_steps: int = 4000, max_epoch: int = 40) -> int:\n",
    "    return min(max_epoch, (max_steps // ((int(task_infos.num_samples[target_task] * 0.8) // batch_size) + 1)) + 1)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:32.393287Z",
     "start_time": "2025-05-19T10:20:32.380434Z"
    }
   },
   "source": [
    "# baselines\n",
    "# these are the default options for all tasks, they should not be modified without justification\n",
    "def get_default_config(target_task: str, shrunk: bool = False) -> Dict[str, Union[str, bool, int]]:\n",
    "    if shrunk:\n",
    "        epochs = 40\n",
    "    else:\n",
    "        epochs = optimize_epochs(target_task=target_task, batch_size=300, max_steps=4000, max_epoch=40)\n",
    "    default_options = {'tasks': 'pami', 'pivot.name': t, 'mode.cv': False, 'mode.nested': False,\n",
    "                       'mode.store_parameters': False, 'sampling.balanced': True,\n",
    "                       'sampling.batch_size': 300, 'callbacks': 'none', 'lr_scheduler': 'step',\n",
    "                       '+trainer.check_val_every_n_epoch': epochs,\n",
    "                       'trainer.max_epochs': epochs, 'augmentations': 'baseline256', 'sampling.enable_caching': True}\n",
    "    return default_options\n",
    "\n",
    "\n",
    "base_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in all_tasks:\n",
    "        opts = get_default_config(t)\n",
    "        opts.update({'proj': f'{projects[\"raw_baseline\"]}_{ix}', 'seed': ix, 'mode.store_parameters': True})\n",
    "        base_cmds.append(MMLJobDescription(prefix_req=def_reqs, mode='train', config_options=opts))\n",
    "        if t in shrinkable_tasks:\n",
    "            shrink_opts = get_default_config(t, shrunk=True)\n",
    "            shrink_opts.update({'proj': f'{projects[\"raw_shrunk\"]}_{ix}', 'tasks': 'pami_shrink'})\n",
    "            base_cmds.append(MMLJobDescription(prefix_req=def_reqs, mode='train', config_options=shrink_opts))\n",
    "mml.interactive.write_out_commands(cmd_list=base_cmds, name='baseline')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 381 commands at baseline.txt.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:32.750763Z",
     "start_time": "2025-05-19T10:20:32.635343Z"
    }
   },
   "source": [
    "#################################\n",
    "# EXPERIMENT 1: MODEL TRANSFER  #\n",
    "#################################\n",
    "# VRAM requirements for timm architectures\n",
    "model_transfer_arch_reqs = {\n",
    "    'tf_efficientnet_b2': 23.0,\n",
    "    'tf_efficientnet_b2_ap': 24.0,\n",
    "    'tf_efficientnet_b2_ns': 24.0,\n",
    "    'tf_efficientnet_cc_b0_4e': 22.0,\n",
    "    'swsl_resnet50': 20.0,\n",
    "    'ssl_resnext50_32x4d': 24.0,\n",
    "    'regnetx_032': 20.5,\n",
    "    'regnety_032': 22.0,\n",
    "    'rexnet_100': 20.5,\n",
    "    'ecaresnet50d': 24.0,\n",
    "    'cspdarknet53': 23.0,\n",
    "    'mixnet_l': 25.0,\n",
    "    'cspresnext50': 24.0,\n",
    "    'cspresnet50': 18.0,\n",
    "    'ese_vovnet39b': 25.0,\n",
    "    'resnest50d': 25.5,\n",
    "    'hrnet_w18': 24.0,\n",
    "    'skresnet34': 16.5,\n",
    "    'mobilenetv3_large_100': 13.5,\n",
    "    'res2net50_26w_4s': 24.5\n",
    "}\n",
    "arch_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in source_tasks:\n",
    "        for arch, vram in model_transfer_arch_reqs.items():\n",
    "            opts = get_default_config(t)\n",
    "            opts.update({'proj': f'{projects[\"arch_search\"]}_{ix}',\n",
    "                         'arch.name': arch, 'seed': ix})\n",
    "            # the following goes back to a rare occurrence of incompatible singleton batches with some batch_norms\n",
    "            # avoid this by minimally wiggle batch size\n",
    "            if t == 'mura_xr_wrist' and arch in ['rexnet_100', 'resnest50d', 'skresnet34']:\n",
    "                opts.update({'sampling.batch_size': 301})\n",
    "            if CLUSTER_USAGE:\n",
    "                arch_reqs = dataclasses.replace(def_reqs, vram_per_gpu=vram)\n",
    "            else:\n",
    "                arch_reqs = def_reqs\n",
    "            arch_cmds.append(MMLJobDescription(prefix_req=arch_reqs, mode='train',\n",
    "                                               config_options=opts))\n",
    "mml.interactive.write_out_commands(cmd_list=arch_cmds, name='full_arch', max_cmds=2000)\n",
    "arch_shrunk_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        if task_infos.num_classes[t] > 40 or task_infos.num_samples[t] <= 1000:\n",
    "            continue\n",
    "        for arch, vram in model_transfer_arch_reqs.items():\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update({'proj': f'{projects[\"arch_infer\"]}_{ix}', 'tasks': 'pami_shrink',\n",
    "                         'arch.name': arch, 'seed': ix})\n",
    "            if CLUSTER_USAGE:\n",
    "                arch_reqs = dataclasses.replace(def_reqs, vram_per_gpu=vram)\n",
    "            else:\n",
    "                arch_reqs = def_reqs\n",
    "            arch_shrunk_cmds.append(MMLJobDescription(prefix_req=arch_reqs, mode='train',\n",
    "                                                         config_options=opts))\n",
    "mml.interactive.write_out_commands(cmd_list=arch_shrunk_cmds, name='arch_shrunk', max_cmds=2000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at full_arch_0.txt.\n",
      "Stored 2000 commands at full_arch_1.txt.\n",
      "Stored 260 commands at full_arch_2.txt.\n",
      "Stored 2000 commands at arch_shrunk_0.txt.\n",
      "Stored 160 commands at arch_shrunk_1.txt.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:33.039193Z",
     "start_time": "2025-05-19T10:20:32.827482Z"
    }
   },
   "source": [
    "####################################\n",
    "# EXPERIMENT 2: TRANSFER LEARNING  #\n",
    "####################################\n",
    "trans_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # only small tasks are used as targets\n",
    "        if task_infos.num_classes[t] > 40:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            mod_task_file = 'pami' if task_infos.num_samples[t] <= 1000 else 'pami_shrink'\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update({'proj': f'{projects[\"transfer\"]}_{ix}', 'tasks': mod_task_file,\n",
    "                         'reuse.models': f'{projects[\"raw_baseline\"]}_{ix}', 'mode.pretrain_task': s,\n",
    "                         'seed': ix})\n",
    "            trans_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='tl'))\n",
    "mml.interactive.write_out_commands(cmd_list=trans_cmds, name='transfer', max_cmds=2000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at transfer_0.txt.\n",
      "Stored 2000 commands at transfer_1.txt.\n",
      "Stored 2000 commands at transfer_2.txt.\n",
      "Stored 2000 commands at transfer_3.txt.\n",
      "Stored 496 commands at transfer_4.txt.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:33.371584Z",
     "start_time": "2025-05-19T10:20:33.164206Z"
    }
   },
   "source": [
    "######################################\n",
    "# EXPERIMENT 3: AUG POLICY TRANSFER  #\n",
    "######################################\n",
    "# Step 1:  training the auto augmentation pipeline for each potential source\n",
    "if not all([(Path(os.getenv('MML_RESULTS_PATH')) / (projects['aa_search'] + f'_{ix}')).exists() for ix in range(rerun)]):\n",
    "    raise RuntimeError(f\"AA mode is not supported anymore with the recent version of MML, you need to import the following projects manually -> pami2_t_aa_search_01_0, pami2_t_aa_search_01_1 and pami2_t_aa_search_01_2 from the data/auto_augmentations folder. Put these to your MML results folder at {os.getenv('MML_RESULTS_PATH')}.\")\n",
    "# Step 2: evaluating the augmentation pipeline\n",
    "policy_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # only small tasks are used as targets\n",
    "        if task_infos.num_classes[t] > 40:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            mod_task_file = 'pami' if task_infos.num_samples[t] <= 1000 else 'pami_shrink'\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update({'proj': f'{projects[\"aa_infer\"]}_{ix}', 'tasks': mod_task_file,\n",
    "                         '+reuse.aa': f'{projects[\"aa_search\"]}_{ix}',\n",
    "                         'augmentations': 'load_aa_from',\n",
    "                         'augmentations.source': s, 'seed': ix})\n",
    "            # note that we use the aatrain mode here to inject the augmentation\n",
    "            policy_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='aatrain'))\n",
    "mml.interactive.write_out_commands(cmd_list=policy_cmds, name='policy', max_cmds=2000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at policy_0.txt.\n",
      "Stored 2000 commands at policy_1.txt.\n",
      "Stored 2000 commands at policy_2.txt.\n",
      "Stored 2000 commands at policy_3.txt.\n",
      "Stored 496 commands at policy_4.txt.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:34.165352Z",
     "start_time": "2025-05-19T10:20:33.631143Z"
    }
   },
   "source": [
    "######################################\n",
    "# EXPERIMENT 4: MULTI-TASK LEARNING  #\n",
    "######################################\n",
    "# We did not use full multitask learning with full sized target tasks in the paper (except for small tasks)\n",
    "multi_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        for s in get_valid_sources(t):\n",
    "            opts = get_default_config(t)\n",
    "            opts.update(\n",
    "                {\n",
    "                    'proj': f'{projects[\"multi_task\"]}_{ix}',\n",
    "                    'mode.multitask': 2,\n",
    "                    'sampling.balanced': True,\n",
    "                    'mode.co_tasks': [s],\n",
    "                    'sampling.sample_num': int(0.8 * task_infos.num_samples[t]),\n",
    "                    'loss.auto_activate_weighing': False, 'seed': ix})\n",
    "            multi_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='train'))\n",
    "mml.interactive.write_out_commands(cmd_list=multi_cmds, name='full_multi', max_cmds=2000)\n",
    "\n",
    "multi_shrunk_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # unshrinkable or already covered above\n",
    "        if task_infos.num_classes[t] > 40 or task_infos.num_samples[t] <= 1000:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update(\n",
    "                {'tasks': 'pami_shrink',\n",
    "                 'proj': f'{projects[\"multi_shrunk\"]}_{ix}',\n",
    "                 'mode.multitask': 2,\n",
    "                 'sampling.balanced': True,\n",
    "                 'mode.co_tasks': [s],\n",
    "                 'sampling.sample_num': min(int(0.8 * task_infos.num_samples[t]), 800),\n",
    "                 'loss.auto_activate_weighing': False, 'seed': ix})\n",
    "            multi_shrunk_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='train'))\n",
    "mml.interactive.write_out_commands(cmd_list=multi_shrunk_cmds, name='multi_shrunk', max_cmds=2000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at full_multi_0.txt.\n",
      "Stored 2000 commands at full_multi_1.txt.\n",
      "Stored 2000 commands at full_multi_2.txt.\n",
      "Stored 2000 commands at full_multi_3.txt.\n",
      "Stored 496 commands at full_multi_4.txt.\n",
      "Stored 2000 commands at multi_shrunk_0.txt.\n",
      "Stored 2000 commands at multi_shrunk_1.txt.\n",
      "Stored 2000 commands at multi_shrunk_2.txt.\n",
      "Stored 1026 commands at multi_shrunk_3.txt.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:34.337366Z",
     "start_time": "2025-05-19T10:20:34.333076Z"
    }
   },
   "source": [
    "all_train_cmds = base_cmds + arch_cmds + arch_shrunk_cmds + trans_cmds + policy_cmds + multi_shrunk_cmds\n",
    "print(f'Our experiments trained {len(all_train_cmds)} models.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our experiments trained 30819 models.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:34.696876Z",
     "start_time": "2025-05-19T10:20:34.692440Z"
    }
   },
   "source": [
    "# if you want to submit jobs to the cluster or run them locally, consider the runner functionality\n",
    "# see mml_lsf README instructions on how to set this up \n",
    "# the following demonstrates submission of the baseline jobs\n",
    "if CLUSTER_USAGE:\n",
    "    from mml_lsf.runner import LSFJobRunner\n",
    "\n",
    "    runner = LSFJobRunner()\n",
    "    for job in base_cmds:\n",
    "        runner.run(job)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:35.845788Z",
     "start_time": "2025-05-19T10:20:35.841143Z"
    }
   },
   "source": [
    "# after running all experiments results can be transferred back with these retrieve commands\n",
    "if CLUSTER_USAGE:\n",
    "    sync_cmds = list()\n",
    "    for ix in range(rerun):\n",
    "        for proj_id in ['multi_task', 'aa_infer', 'transfer', 'arch_search', 'raw_shrunk',\n",
    "                        'raw_baseline', 'multi_shrunk', 'arch_infer']:\n",
    "            sync_cmds.append(get_retrieve_for_proj(f'{projects[proj_id]}_{ix}'))\n",
    "    with open(Path(os.path.abspath('')) / 'output_sync.txt', 'w') as file:\n",
    "        file.write('\\n'.join(sync_cmds))\n",
    "    print(f'Stored {len(sync_cmds)} commands at output_sync.txt.')"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Feature and FIM extraction\n",
    "\n",
    "This is how task feature extraction works. Note that full features comprise several GB and are not provided directly (also for licensing compatibility issues). The computed task distances are provided in the `cache` folder top-level."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-19T10:20:47.945301Z",
     "start_time": "2025-05-19T10:20:47.941182Z"
    }
   },
   "source": [
    "updated_shrunk_task_list = [t.replace(' --shrink_train 800', '+shrink_train?800') for t in all_tasks_including_shrunk]\n",
    "\n",
    "features_cmd = MMLJobDescription(prefix_req=def_reqs,\n",
    "                                 config_options={'task_list': updated_shrunk_task_list, 'proj': 'pami2_features',\n",
    "                                                 'distance': 'emd', \n",
    "                                                 'distance._mode.subroutines': ['feature'], 'augmentations': 'baseline256'},\n",
    "                                 mode='similarity')\n",
    "fim_cmd = MMLJobDescription(prefix_req=def_reqs,\n",
    "                            config_options={'task_list': updated_shrunk_task_list, 'proj': 'pami2_fims_recent', 'distance': 'fed', \n",
    "                                            'distance._mode.subroutines': ['tune', 'fim'], 'sampling.sample_num': 8000,\n",
    "                                            'sampling.balanced': True, 'distance.fim.samples': 2000,\n",
    "                                            'augmentations': 'baseline256', }, mode='similarity')"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T16:01:42.867708Z",
     "start_time": "2025-05-19T16:01:42.862168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# additional exps (revision)\n",
    "extraction_models = ['vit_base_patch16_224.dino', 'vit_base_patch16_clip_224.openai', 'vit_base_patch16_224.mae']\n",
    "meths = ['dino', 'clip', 'mae']\n",
    "generalization_cmds = []\n",
    "for model, meth in zip(extraction_models, meths):\n",
    "    # extract features for generalization backbone\n",
    "    cmd = MMLJobDescription(prefix_req=def_reqs,\n",
    "                                 config_options={'task_list': updated_shrunk_task_list, 'proj': 'pami2_features_' + meth,\n",
    "                                                 'distance': 'emd', 'arch.name': model,\n",
    "                                                 'distance._mode.subroutines': ['feature'], 'augmentations': 'baseline256'},\n",
    "                                 mode='similarity')\n",
    "    generalization_cmds.append(cmd)\n",
    "    # extract FIMs for generalization backbone (was skipped for computational complexity reasons)\n",
    "    # cmd = MMLJobDescription(prefix_req=def_reqs,\n",
    "    #                             config_options={'task_list': updated_shrunk_task_list, 'proj': 'pami2_fims_' + meth,\n",
    "    #                                              'distance': 'fed', 'arch.name': model, 'sampling.sample_num': 8000,\n",
    "    #                                         'sampling.balanced': True, 'distance.fim.samples': 2000, '+reuse.fc_tuned': 'pami2_fims_' + meth, 'sampling.batch_size': 1, 'distance.fim.nngeom': False,\n",
    "    #                                              'distance._mode.subroutines': ['fim'], 'augmentations': 'baseline256'},\n",
    "    #                             mode='similarity')\n",
    "    # generalization_cmds.append(cmd)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "is_executing": true
    }
   },
   "source": [
    "# the following demonstrates how to run these locally from within this notebook\n",
    "# CAUTION: it produces a lot of logging output to the notebook - consider running these commands in the terminal as described below\n",
    "from mml.interactive import SubprocessJobRunner\n",
    "\n",
    "local_reqs = DefaultRequirements()\n",
    "runner = SubprocessJobRunner()\n",
    "for job in [features_cmd, fim_cmd] + generalization_cmds:\n",
    "    job.prefix_req = local_reqs\n",
    "    # runner.run(job)  # uncomment to run"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-05-12T13:52:55.588762Z",
     "start_time": "2025-05-12T13:52:55.580177Z"
    }
   },
   "source": [
    "# want to run in the terminal - follow here\n",
    "local_reqs = DefaultRequirements()\n",
    "for job in [features_cmd, fim_cmd] + generalization_cmds:\n",
    "    job.prefix_req = local_reqs\n",
    "features_cmd.render()  # paste the output into terminal (remove surrounding quotes) takes ~20 minutes"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mml similarity task_list=['lapgyn4_anatomical_structures','lapgyn4_surgical_actions','lapgyn4_instrument_count','lapgyn4_anatomical_actions','sklin2_skin_lesions','identify_nbi_infframes','laryngeal_tissues','nerthus_bowel_cleansing_quality','stanford_dogs_image_categorization','svhn','caltech101_object_classification','caltech256_object_classification','cifar10_object_classification','cifar100_object_classification','mnist_digit_classification','emnist_digit_classification','hyperkvasir_anatomical-landmarks','hyperkvasir_pathological-findings','hyperkvasir_quality-of-mucosal-views','hyperkvasir_therapeutic-interventions','cholec80_grasper_presence','cholec80_bipolar_presence','cholec80_hook_presence','cholec80_scissors_presence','cholec80_clipper_presence','cholec80_irrigator_presence','cholec80_specimenbag_presence','derm7pt_skin_lesions','idle_action_recognition','barretts_esophagus_diagnosis','brain_tumor_classification','mednode_melanoma_classification','brain_tumor_type_classification','chexpert_enlarged_cardiomediastinum','chexpert_cardiomegaly','chexpert_lung_opacity','chexpert_lung_lesion','chexpert_edema','chexpert_consolidation','chexpert_pneumonia','chexpert_atelectasis','chexpert_pneumothorax','chexpert_pleural_effusion','chexpert_pleural_other','chexpert_fracture','chexpert_support_devices','pneumonia_classification','ph2-melanocytic-lesions-classification','covid_xray_classification','isic20_melanoma_classification','deep_drid_dr_level','shenzen_chest_xray_tuberculosis','crawled_covid_ct_classification','deep_drid_quality','deep_drid_clarity','deep_drid_field','deep_drid_artifact','kvasir_capsule_anatomy','kvasir_capsule_content','kvasir_capsule_pathologies','breast_cancer_classification_v2','eye_condition_classification','mura_xr_wrist','mura_xr_shoulder','mura_xr_humerus','mura_xr_hand','mura_xr_forearm','mura_xr_finger','mura_xr_elbow','bean_plant_disease_classification','aptos19_blindness_detection','lapgyn4_anatomical_structures+shrink_train?800','lapgyn4_surgical_actions+shrink_train?800','lapgyn4_instrument_count+shrink_train?800','lapgyn4_anatomical_actions+shrink_train?800','laryngeal_tissues+shrink_train?800','nerthus_bowel_cleansing_quality+shrink_train?800','svhn+shrink_train?800','cifar10_object_classification+shrink_train?800','mnist_digit_classification+shrink_train?800','hyperkvasir_anatomical-landmarks+shrink_train?800','hyperkvasir_pathological-findings+shrink_train?800','hyperkvasir_quality-of-mucosal-views+shrink_train?800','hyperkvasir_therapeutic-interventions+shrink_train?800','cholec80_grasper_presence+shrink_train?800','cholec80_bipolar_presence+shrink_train?800','cholec80_hook_presence+shrink_train?800','cholec80_scissors_presence+shrink_train?800','cholec80_clipper_presence+shrink_train?800','cholec80_irrigator_presence+shrink_train?800','cholec80_specimenbag_presence+shrink_train?800','idle_action_recognition+shrink_train?800','brain_tumor_classification+shrink_train?800','brain_tumor_type_classification+shrink_train?800','chexpert_enlarged_cardiomediastinum+shrink_train?800','chexpert_cardiomegaly+shrink_train?800','chexpert_lung_opacity+shrink_train?800','chexpert_lung_lesion+shrink_train?800','chexpert_edema+shrink_train?800','chexpert_consolidation+shrink_train?800','chexpert_pneumonia+shrink_train?800','chexpert_atelectasis+shrink_train?800','chexpert_pneumothorax+shrink_train?800','chexpert_pleural_effusion+shrink_train?800','chexpert_pleural_other+shrink_train?800','chexpert_fracture+shrink_train?800','chexpert_support_devices+shrink_train?800','pneumonia_classification+shrink_train?800','covid_xray_classification+shrink_train?800','isic20_melanoma_classification+shrink_train?800','deep_drid_dr_level+shrink_train?800','deep_drid_quality+shrink_train?800','deep_drid_clarity+shrink_train?800','deep_drid_field+shrink_train?800','deep_drid_artifact+shrink_train?800','kvasir_capsule_anatomy+shrink_train?800','kvasir_capsule_content+shrink_train?800','kvasir_capsule_pathologies+shrink_train?800','mura_xr_wrist+shrink_train?800','mura_xr_shoulder+shrink_train?800','mura_xr_humerus+shrink_train?800','mura_xr_hand+shrink_train?800','mura_xr_forearm+shrink_train?800','mura_xr_finger+shrink_train?800','mura_xr_elbow+shrink_train?800','bean_plant_disease_classification+shrink_train?800','aptos19_blindness_detection+shrink_train?800'] proj=pami2_features distance=emd distance._mode.subroutines=['feature'] augmentations=baseline256\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-repro",
   "language": "python",
   "name": "tf-repro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
