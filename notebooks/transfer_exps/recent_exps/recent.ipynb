{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:41.061650Z",
     "start_time": "2025-03-12T10:47:41.056261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _____ ______   _____ ______   ___\n",
      "|\\   _ \\  _   \\|\\   _ \\  _   \\|\\  \\\n",
      "\\ \\  \\\\\\__\\ \\  \\ \\  \\\\\\__\\ \\  \\ \\  \\\n",
      " \\ \\  \\\\|__| \\  \\ \\  \\\\|__| \\  \\ \\  \\\n",
      "  \\ \\  \\    \\ \\  \\ \\  \\    \\ \\  \\ \\  \\____\n",
      "   \\ \\__\\    \\ \\__\\ \\__\\    \\ \\__\\ \\_______\\\n",
      "    \\|__|     \\|__|\\|__|     \\|__|\\|_______|\n",
      "         ____  _  _    __  _  _  ____  _  _\n",
      "        (  _ \\( \\/ )  (  )( \\/ )/ ___)( \\/ )\n",
      "         ) _ ( )  /    )( / \\/ \\\\___ \\ )  /\n",
      "        (____/(__/    (__)\\_)(_/(____/(__/\n",
      "Interactive MML API initialized.\n"
     ]
    }
   ],
   "source": [
    "# this notebook generates all commands for the recent mml version \n",
    "import dataclasses\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Union\n",
    "\n",
    "try:\n",
    "    import mml.interactive\n",
    "except ImportError:\n",
    "    raise RuntimeError('This reproduction expects a recent version of MML - please refer to the README for detailed instructions.')\n",
    "\n",
    "mml.interactive.init(Path('~/.config/mml.env').expanduser())\n",
    "from mml.interactive import DefaultRequirements, MMLJobDescription\n",
    "from mml_tf.tasks import all_tasks, get_valid_sources, shrinkable_tasks, target_tasks, source_tasks, train_tasks, \\\n",
    "    all_tasks_including_shrunk, task_infos\n",
    "\n",
    "CLUSTER_USAGE = False  # change if you (want / do not want) to run on the cluster\n",
    "\n",
    "if CLUSTER_USAGE:\n",
    "    from mml_lsf.requirements import LSFSubmissionRequirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:41.117477Z",
     "start_time": "2025-03-12T10:47:41.115012Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# note that final experiments have to be run multiple times to ensure significance\n",
    "rerun = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:41.169342Z",
     "start_time": "2025-03-12T10:47:41.163691Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if CLUSTER_USAGE:\n",
    "    # cluster submission prepends, add yours here in case you have other gpu requirements\n",
    "    base_reqs = LSFSubmissionRequirements(special_requirements=[],\n",
    "                                          undesired_hosts=['e230-dgx2-2', 'e230-dgxa100-2', 'e230-dgxa100-4',\n",
    "                                                           'e230-dgxa100-1',\n",
    "                                                           'e230-dgxa100-2', 'e230-dgxa100-3', 'e230-dgxa100-4',\n",
    "                                                           'lsf22-gpu08', 'lsf22-gpu01', 'lsf22-gpu02', 'lsf22-gpu03',\n",
    "                                                           'lsf22-gpu04', 'lsf22-gpu05', 'lsf22-gpu06', 'lsf22-gpu07'],\n",
    "                                          num_gpus=1, vram_per_gpu=11.0, queue='gpu-lowprio',\n",
    "                                          mail='EMAIL.ADDRESS@dkfz-heidelberg.de', script_name='mml.sh',\n",
    "                                          job_group='/USERNAME/pami_rerun'\n",
    "                                          )\n",
    "    # see for example this local setup\n",
    "    # base_reqs = pp_reqs = aa_reqs = def_reqs = arch_reqs = tl_reqs = multi_reqs = nb.DefaultRequirements()\n",
    "    pp_reqs = dataclasses.replace(base_reqs, queue='gpu')\n",
    "    aa_reqs = dataclasses.replace(base_reqs, script_name='aa.sh', vram_per_gpu=13.0)\n",
    "    def_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'])\n",
    "    tl_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'], vram_per_gpu=24.0)\n",
    "    multi_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'], vram_per_gpu=14.0)\n",
    "else:\n",
    "    base_reqs = pp_reqs = aa_reqs = def_reqs = tl_reqs = multi_reqs = DefaultRequirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:41.218522Z",
     "start_time": "2025-03-12T10:47:41.215232Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# project overview -> points to MML projects we use, we will append indices for each \"rerun\"\n",
    "projects = {\n",
    "    'base': 'pami2_base_02',\n",
    "    'raw_baseline': 'pami2_raw_03',\n",
    "    'raw_shrunk': 'pami2_raw_shrunk_10',\n",
    "    # aa search can not be carried out with recent MML version, we provide the created policies in data/auto_augmentations\n",
    "    'aa_search': 'pami2_t_aa_search_01',\n",
    "    # the above are shared with train (!) since stuff is only computed once anyway\n",
    "    'transfer': 'pami2_t_transfer_20',\n",
    "    'multi_task': 'test_multi_balanced_test_split_10', \n",
    "    'multi_shrunk': 'test_multi_balanced_shrunk_test_split_10',\n",
    "    'arch_search': 'pami2_t_arch_search_02',\n",
    "    'arch_infer': 'pami2_t_arch_infer_02',\n",
    "    'aa_infer': 'pami2_t_aa_infer_02'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:41.275962Z",
     "start_time": "2025-03-12T10:47:41.270352Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 3 commands at prepare.txt.\n"
     ]
    }
   ],
   "source": [
    "# prepare steps\n",
    "prep_cmds = list()\n",
    "# step one: plain task creation\n",
    "\n",
    "prep_cmds.append(MMLJobDescription(prefix_req=pp_reqs, mode='create',\n",
    "                                   config_options={'tasks': 'pami', 'proj': projects['base']}))\n",
    "# step two: plain task preprocessing\n",
    "prep_cmds.append(MMLJobDescription(prefix_req=pp_reqs, mode='pp',\n",
    "                                   config_options={'tasks': 'pami', 'proj': projects['base']}))\n",
    "# step three: shrunk preprocessing\n",
    "prep_cmds.append(MMLJobDescription(prefix_req=pp_reqs, mode='info',\n",
    "                                   config_options={'tasks': 'pami_shrinkable_800', 'proj': projects['base']}))\n",
    "mml.interactive.write_out_commands(cmd_list=prep_cmds, name='prepare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:41.335996Z",
     "start_time": "2025-03-12T10:47:41.332128Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 1 commands at dimensions.txt.\n"
     ]
    }
   ],
   "source": [
    "# OPTIONALLY: compute dimensions (used for Fig. 3) and some additional experiments not shown in the paper\n",
    "dim_cmds = list()\n",
    "dim_cmds.append(MMLJobDescription(prefix_req=def_reqs, mode='dim', config_options={'tasks': 'pami_shrink_mix',\n",
    "                                                                                   'proj': projects[\"base\"],\n",
    "                                                                                   'mode.inv_mle': True}))\n",
    "mml.interactive.write_out_commands(cmd_list=dim_cmds, name='dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:41.411283Z",
     "start_time": "2025-03-12T10:47:41.407110Z"
    }
   },
   "outputs": [],
   "source": [
    "# convenience function for easier retrieve from cluster results, \n",
    "user_id = 'USERNAME'  \n",
    "# use as print(get_retrieve_for_proj('my_project')) and run the result in a shell to get the results of 'my_project' from cluster to your local system\n",
    "def get_retrieve_for_proj(proj):\n",
    "    return f\"rsync -rtvu --stats --exclude=PARAMETERS --exclude=hpo --exclude=runs --exclude=FIMS --exclude=FC_TUNED {user_id}@odcf-worker01:{os.getenv('MML_CLUSTER_RESULTS_PATH')}/{proj}/ {os.getenv('MML_RESULTS_PATH')}/{proj}\"\n",
    "\n",
    "\n",
    "# the following optimizes a jobs epochs in a way that target task is seen at least 40 epochs but at max 4000 steps (plus finishing the epoch)\n",
    "def optimize_epochs(target_task: str, batch_size: int = 300, max_steps: int = 4000, max_epoch: int = 40) -> int:\n",
    "    return min(max_epoch, (max_steps // ((int(task_infos.num_samples[target_task] * 0.8) // batch_size) + 1)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:41.473979Z",
     "start_time": "2025-03-12T10:47:41.461814Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 381 commands at baseline.txt.\n"
     ]
    }
   ],
   "source": [
    "# baselines\n",
    "# these are the default options for all tasks, they should not be modified without justification\n",
    "def get_default_config(target_task: str, shrunk: bool = False) -> Dict[str, Union[str, bool, int]]:\n",
    "    if shrunk:\n",
    "        epochs = 40\n",
    "    else:\n",
    "        epochs = optimize_epochs(target_task=target_task, batch_size=300, max_steps=4000, max_epoch=40)\n",
    "    default_options = {'tasks': 'pami', 'pivot.name': t, 'mode.cv': False, 'mode.nested': False,\n",
    "                       'mode.store_parameters': False, 'sampling.balanced': True,\n",
    "                       'sampling.batch_size': 300, 'callbacks': 'none', 'lr_scheduler': 'step',\n",
    "                       '+trainer.check_val_every_n_epoch': epochs,\n",
    "                       'trainer.max_epochs': epochs, 'augmentations': 'baseline256', 'sampling.enable_caching': True}\n",
    "    return default_options\n",
    "\n",
    "\n",
    "base_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in all_tasks:\n",
    "        opts = get_default_config(t)\n",
    "        opts.update({'proj': f'{projects[\"raw_baseline\"]}_{ix}', 'seed': ix, 'mode.store_parameters': True})\n",
    "        base_cmds.append(MMLJobDescription(prefix_req=def_reqs, mode='train', config_options=opts))\n",
    "        if t in shrinkable_tasks:\n",
    "            shrink_opts = get_default_config(t, shrunk=True)\n",
    "            shrink_opts.update({'proj': f'{projects[\"raw_shrunk\"]}_{ix}', 'tasks': 'pami_shrink'})\n",
    "            base_cmds.append(MMLJobDescription(prefix_req=def_reqs, mode='train', config_options=shrink_opts))\n",
    "mml.interactive.write_out_commands(cmd_list=base_cmds, name='baseline')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-03-14T13:06:45.026875Z",
     "start_time": "2025-03-14T13:06:44.914447Z"
    }
   },
   "source": [
    "#################################\n",
    "# EXPERIMENT 1: MODEL TRANSFER  #\n",
    "#################################\n",
    "# VRAM requirements for timm architectures\n",
    "model_transfer_arch_reqs = {\n",
    "    'tf_efficientnet_b2': 23.0,\n",
    "    'tf_efficientnet_b2_ap': 24.0,\n",
    "    'tf_efficientnet_b2_ns': 24.0,\n",
    "    'tf_efficientnet_cc_b0_4e': 22.0,\n",
    "    'swsl_resnet50': 20.0,\n",
    "    'ssl_resnext50_32x4d': 24.0,\n",
    "    'regnetx_032': 20.5,\n",
    "    'regnety_032': 22.0,\n",
    "    'rexnet_100': 20.5,\n",
    "    'ecaresnet50d': 24.0,\n",
    "    'cspdarknet53': 23.0,\n",
    "    'mixnet_l': 25.0,\n",
    "    'cspresnext50': 24.0,\n",
    "    'cspresnet50': 18.0,\n",
    "    'ese_vovnet39b': 25.0,\n",
    "    'resnest50d': 25.5,\n",
    "    'hrnet_w18': 24.0,\n",
    "    'skresnet34': 16.5,\n",
    "    'mobilenetv3_large_100': 13.5,\n",
    "    'res2net50_26w_4s': 24.5\n",
    "}\n",
    "arch_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in source_tasks:\n",
    "        for arch, vram in model_transfer_arch_reqs.items():\n",
    "            opts = get_default_config(t)\n",
    "            opts.update({'proj': f'{projects[\"arch_search\"]}_{ix}',\n",
    "                         'arch.name': arch, 'seed': ix})\n",
    "            # the following goes back to a rare occurrence of incompatible singleton batches with some batch_norms\n",
    "            # avoid this by minimally wiggle batch size\n",
    "            if t == 'mura_xr_wrist' and arch in ['rexnet_100', 'resnest50d', 'skresnet34']:\n",
    "                opts.update({'sampling.batch_size': 301})\n",
    "            if CLUSTER_USAGE:\n",
    "                arch_reqs = dataclasses.replace(def_reqs, vram_per_gpu=vram)\n",
    "            else:\n",
    "                arch_reqs = def_reqs\n",
    "            arch_cmds.append(MMLJobDescription(prefix_req=arch_reqs, mode='train',\n",
    "                                               config_options=opts))\n",
    "mml.interactive.write_out_commands(cmd_list=arch_cmds, name='full_arch', max_cmds=2000)\n",
    "arch_shrunk_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        if task_infos.num_classes[t] > 40 or task_infos.num_samples[t] <= 1000:\n",
    "            continue\n",
    "        for arch, vram in model_transfer_arch_reqs.items():\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update({'proj': f'{projects[\"arch_infer\"]}_{ix}', 'tasks': 'pami_shrink',\n",
    "                         'arch.classification.id': arch, 'seed': ix})\n",
    "            if CLUSTER_USAGE:\n",
    "                arch_reqs = dataclasses.replace(def_reqs, vram_per_gpu=vram)\n",
    "            else:\n",
    "                arch_reqs = def_reqs\n",
    "            arch_shrunk_cmds.append(MMLJobDescription(prefix_req=arch_reqs, mode='train',\n",
    "                                                         config_options=opts))\n",
    "mml.interactive.write_out_commands(cmd_list=arch_shrunk_cmds, name='arch_shrunk', max_cmds=2000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at full_arch_0.txt.\n",
      "Stored 2000 commands at full_arch_1.txt.\n",
      "Stored 260 commands at full_arch_2.txt.\n",
      "Stored 2000 commands at arch_shrunk_0.txt.\n",
      "Stored 160 commands at arch_shrunk_1.txt.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:41.855079Z",
     "start_time": "2025-03-12T10:47:41.679177Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at transfer_0.txt.\n",
      "Stored 2000 commands at transfer_1.txt.\n",
      "Stored 2000 commands at transfer_2.txt.\n",
      "Stored 2000 commands at transfer_3.txt.\n",
      "Stored 496 commands at transfer_4.txt.\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# EXPERIMENT 2: TRANSFER LEARNING  #\n",
    "####################################\n",
    "trans_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # only small tasks are used as targets\n",
    "        if task_infos.num_classes[t] > 40:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            mod_task_file = 'pami' if task_infos.num_samples[t] <= 1000 else 'pami_shrink'\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update({'proj': f'{projects[\"transfer\"]}_{ix}', 'tasks': mod_task_file,\n",
    "                         'reuse.models': f'{projects[\"raw_baseline\"]}_{ix}', 'mode.pretrain_task': s,\n",
    "                         'seed': ix})\n",
    "            trans_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='tl'))\n",
    "mml.interactive.write_out_commands(cmd_list=trans_cmds, name='transfer', max_cmds=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:49:11.258075Z",
     "start_time": "2025-03-12T10:49:11.073519Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at policy_0.txt.\n",
      "Stored 2000 commands at policy_1.txt.\n",
      "Stored 2000 commands at policy_2.txt.\n",
      "Stored 2000 commands at policy_3.txt.\n",
      "Stored 496 commands at policy_4.txt.\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "# EXPERIMENT 3: AUG POLICY TRANSFER  #\n",
    "######################################\n",
    "# Step 1:  training the auto augmentation pipeline for each potential source\n",
    "if not all([(Path(os.getenv('MML_RESULTS_PATH')) / (projects['aa_search'] + f'_{ix}')).exists() for ix in range(rerun)]):\n",
    "    raise RuntimeError(f\"AA mode is not supported anymore with the recent version of MML, you need to import the following projects manually -> pami2_t_aa_search_01_0, pami2_t_aa_search_01_1 and pami2_t_aa_search_01_2 from the data/auto_augmentations folder. Put these to your MML results folder at {os.getenv('MML_RESULTS_PATH')}.\")\n",
    "# Step 2: evaluating the augmentation pipeline\n",
    "policy_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # only small tasks are used as targets\n",
    "        if task_infos.num_classes[t] > 40:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            mod_task_file = 'pami' if task_infos.num_samples[t] <= 1000 else 'pami_shrink'\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update({'proj': f'{projects[\"aa_infer\"]}_{ix}', 'tasks': mod_task_file,\n",
    "                         '+reuse.aa': f'{projects[\"aa_search\"]}_{ix}',\n",
    "                         'augmentations': 'load_aa_from',\n",
    "                         'augmentations.source': s, 'seed': ix})\n",
    "            # note that we use the aatrain mode here to inject the augmentation\n",
    "            policy_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='aatrain'))\n",
    "mml.interactive.write_out_commands(cmd_list=policy_cmds, name='policy', max_cmds=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:42.553714Z",
     "start_time": "2025-03-12T10:47:42.108436Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at full_multi_0.txt.\n",
      "Stored 2000 commands at full_multi_1.txt.\n",
      "Stored 2000 commands at full_multi_2.txt.\n",
      "Stored 2000 commands at full_multi_3.txt.\n",
      "Stored 496 commands at full_multi_4.txt.\n",
      "Stored 2000 commands at multi_shrunk_0.txt.\n",
      "Stored 2000 commands at multi_shrunk_1.txt.\n",
      "Stored 2000 commands at multi_shrunk_2.txt.\n",
      "Stored 1026 commands at multi_shrunk_3.txt.\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "# EXPERIMENT 4: MULTI-TASK LEARNING  #\n",
    "######################################\n",
    "# We did not use full multitask learning with full sized target tasks in the paper (except for small tasks)\n",
    "multi_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        for s in get_valid_sources(t):\n",
    "            opts = get_default_config(t)\n",
    "            opts.update(\n",
    "                {\n",
    "                    'proj': f'{projects[\"multi_task\"]}_{ix}',\n",
    "                    'mode.multitask': 2,\n",
    "                    'sampling.balanced': True,\n",
    "                    'mode.co_tasks': [s],\n",
    "                    'sampling.sample_num': int(0.8 * task_infos.num_samples[t]),\n",
    "                    'loss.auto_activate_weighing': False, 'seed': ix})\n",
    "            multi_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='train'))\n",
    "mml.interactive.write_out_commands(cmd_list=multi_cmds, name='full_multi', max_cmds=2000)\n",
    "\n",
    "multi_shrunk_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # unshrinkable or already covered above\n",
    "        if task_infos.num_classes[t] > 40 or task_infos.num_samples[t] <= 1000:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update(\n",
    "                {'tasks': 'pami_shrink',\n",
    "                 'proj': f'{projects[\"multi_shrunk\"]}_{ix}',\n",
    "                 'mode.multitask': 2,\n",
    "                 'sampling.balanced': True,\n",
    "                 'mode.co_tasks': [s],\n",
    "                 'sampling.sample_num': min(int(0.8 * task_infos.num_samples[t]), 800),\n",
    "                 'loss.auto_activate_weighing': False, 'seed': ix})\n",
    "            multi_shrunk_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='train'))\n",
    "mml.interactive.write_out_commands(cmd_list=multi_shrunk_cmds, name='multi_shrunk', max_cmds=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:42.610504Z",
     "start_time": "2025-03-12T10:47:42.605952Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our experiments trained 30819 models.\n"
     ]
    }
   ],
   "source": [
    "all_train_cmds = base_cmds + arch_cmds + arch_shrunk_cmds + trans_cmds + policy_cmds + multi_shrunk_cmds\n",
    "print(f'Our experiments trained {len(all_train_cmds)} models.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:42.701556Z",
     "start_time": "2025-03-12T10:47:42.698248Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if you want to submit jobs to the cluster or run them locally, consider the runner functionality\n",
    "# see mml_lsf README instructions on how to set this up \n",
    "# the following demonstrates submission of the baseline jobs\n",
    "if CLUSTER_USAGE:\n",
    "    from mml_lsf.runner import LSFJobRunner\n",
    "\n",
    "    runner = LSFJobRunner()\n",
    "    for job in base_cmds:\n",
    "        runner.run(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:42.752399Z",
     "start_time": "2025-03-12T10:47:42.747809Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# after running all experiments results can be transferred back with these retrieve commands\n",
    "if CLUSTER_USAGE:\n",
    "    sync_cmds = list()\n",
    "    for ix in range(rerun):\n",
    "        for proj_id in ['multi_task', 'aa_infer', 'transfer', 'arch_search', 'raw_shrunk',\n",
    "                        'raw_baseline', 'multi_shrunk', 'arch_infer']:\n",
    "            sync_cmds.append(get_retrieve_for_proj(f'{projects[proj_id]}_{ix}'))\n",
    "    with open(Path(os.path.abspath('')) / 'output_sync.txt', 'w') as file:\n",
    "        file.write('\\n'.join(sync_cmds))\n",
    "    print(f'Stored {len(sync_cmds)} commands at output_sync.txt.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Feature and FIM extraction\n",
    "\n",
    "This is how task feature extraction works. Note that full features comprise several GB and are not provided directly (also for licensing compatibility issues). The computed task distances are provided in the `cache` folder top-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:42.800485Z",
     "start_time": "2025-03-12T10:47:42.795908Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "updated_shrunk_task_list = [t.replace(' --shrink_train 800', '+shrink_train?800') for t in all_tasks_including_shrunk]\n",
    "\n",
    "features_cmd = MMLJobDescription(prefix_req=def_reqs,\n",
    "                                 config_options={'task_list': updated_shrunk_task_list, 'proj': 'pami2_features',\n",
    "                                                 'distance': 'emd', \n",
    "                                                 'distance._mode.subroutines': ['feature'], 'augmentations': 'baseline256'},\n",
    "                                 mode='similarity')\n",
    "fim_cmd = MMLJobDescription(prefix_req=def_reqs,\n",
    "                            config_options={'task_list': updated_shrunk_task_list, 'proj': 'pami2_fims_recent', 'distance': 'fed', \n",
    "                                            'distance._mode.subroutines': ['tune', 'fim'], 'sampling.sample_num': 8000,\n",
    "                                            'sampling.balanced': True, 'distance.fim.samples': 2000,\n",
    "                                            'augmentations': 'baseline256', }, mode='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:42.854084Z",
     "start_time": "2025-03-12T10:47:42.851096Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# the following demonstrates how to run these locally from within this notebook\n",
    "# CAUTION: it produces a lot of logging output to the notebook - consider running these commands in the terminal as described below\n",
    "from mml.interactive import SubprocessJobRunner\n",
    "\n",
    "local_reqs = DefaultRequirements()\n",
    "runner = SubprocessJobRunner()\n",
    "for job in [features_cmd, fim_cmd]:\n",
    "    job.prefix_req = local_reqs\n",
    "    # runner.run(job)  # uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:42.905601Z",
     "start_time": "2025-03-12T10:47:42.901287Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mml emd task_list=['lapgyn4_anatomical_structures','lapgyn4_surgical_actions','lapgyn4_instrument_count','lapgyn4_anatomical_actions','sklin2_skin_lesions','identify_nbi_infframes','laryngeal_tissues','nerthus_bowel_cleansing_quality','stanford_dogs_image_categorization','svhn','caltech101_object_classification','caltech256_object_classification','cifar10_object_classification','cifar100_object_classification','mnist_digit_classification','emnist_digit_classification','hyperkvasir_anatomical-landmarks','hyperkvasir_pathological-findings','hyperkvasir_quality-of-mucosal-views','hyperkvasir_therapeutic-interventions','cholec80_grasper_presence','cholec80_bipolar_presence','cholec80_hook_presence','cholec80_scissors_presence','cholec80_clipper_presence','cholec80_irrigator_presence','cholec80_specimenbag_presence','derm7pt_skin_lesions','idle_action_recognition','barretts_esophagus_diagnosis','brain_tumor_classification','mednode_melanoma_classification','brain_tumor_type_classification','chexpert_enlarged_cardiomediastinum','chexpert_cardiomegaly','chexpert_lung_opacity','chexpert_lung_lesion','chexpert_edema','chexpert_consolidation','chexpert_pneumonia','chexpert_atelectasis','chexpert_pneumothorax','chexpert_pleural_effusion','chexpert_pleural_other','chexpert_fracture','chexpert_support_devices','pneumonia_classification','ph2-melanocytic-lesions-classification','covid_xray_classification','isic20_melanoma_classification','deep_drid_dr_level','shenzen_chest_xray_tuberculosis','crawled_covid_ct_classification','deep_drid_quality','deep_drid_clarity','deep_drid_field','deep_drid_artifact','kvasir_capsule_anatomy','kvasir_capsule_content','kvasir_capsule_pathologies','breast_cancer_classification_v2','eye_condition_classification','mura_xr_wrist','mura_xr_shoulder','mura_xr_humerus','mura_xr_hand','mura_xr_forearm','mura_xr_finger','mura_xr_elbow','bean_plant_disease_classification','aptos19_blindness_detection','lapgyn4_anatomical_structures+shrink_train?800','lapgyn4_surgical_actions+shrink_train?800','lapgyn4_instrument_count+shrink_train?800','lapgyn4_anatomical_actions+shrink_train?800','laryngeal_tissues+shrink_train?800','nerthus_bowel_cleansing_quality+shrink_train?800','svhn+shrink_train?800','cifar10_object_classification+shrink_train?800','mnist_digit_classification+shrink_train?800','hyperkvasir_anatomical-landmarks+shrink_train?800','hyperkvasir_pathological-findings+shrink_train?800','hyperkvasir_quality-of-mucosal-views+shrink_train?800','hyperkvasir_therapeutic-interventions+shrink_train?800','cholec80_grasper_presence+shrink_train?800','cholec80_bipolar_presence+shrink_train?800','cholec80_hook_presence+shrink_train?800','cholec80_scissors_presence+shrink_train?800','cholec80_clipper_presence+shrink_train?800','cholec80_irrigator_presence+shrink_train?800','cholec80_specimenbag_presence+shrink_train?800','idle_action_recognition+shrink_train?800','brain_tumor_classification+shrink_train?800','brain_tumor_type_classification+shrink_train?800','chexpert_enlarged_cardiomediastinum+shrink_train?800','chexpert_cardiomegaly+shrink_train?800','chexpert_lung_opacity+shrink_train?800','chexpert_lung_lesion+shrink_train?800','chexpert_edema+shrink_train?800','chexpert_consolidation+shrink_train?800','chexpert_pneumonia+shrink_train?800','chexpert_atelectasis+shrink_train?800','chexpert_pneumothorax+shrink_train?800','chexpert_pleural_effusion+shrink_train?800','chexpert_pleural_other+shrink_train?800','chexpert_fracture+shrink_train?800','chexpert_support_devices+shrink_train?800','pneumonia_classification+shrink_train?800','covid_xray_classification+shrink_train?800','isic20_melanoma_classification+shrink_train?800','deep_drid_dr_level+shrink_train?800','deep_drid_quality+shrink_train?800','deep_drid_clarity+shrink_train?800','deep_drid_field+shrink_train?800','deep_drid_artifact+shrink_train?800','kvasir_capsule_anatomy+shrink_train?800','kvasir_capsule_content+shrink_train?800','kvasir_capsule_pathologies+shrink_train?800','mura_xr_wrist+shrink_train?800','mura_xr_shoulder+shrink_train?800','mura_xr_humerus+shrink_train?800','mura_xr_hand+shrink_train?800','mura_xr_forearm+shrink_train?800','mura_xr_finger+shrink_train?800','mura_xr_elbow+shrink_train?800','bean_plant_disease_classification+shrink_train?800','aptos19_blindness_detection+shrink_train?800'] proj=pami2_features mode.subroutines=['feature'] augmentations=baseline256\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# want to run in the terminal - follow here\n",
    "local_reqs = DefaultRequirements()\n",
    "for job in [features_cmd, fim_cmd]:\n",
    "    job.prefix_req = local_reqs\n",
    "features_cmd.render()  # paste the output into terminal (remove surrounding quotes) takes ~20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:47:43.011005Z",
     "start_time": "2025-03-12T10:47:43.006676Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mml fed task_list=['lapgyn4_anatomical_structures','lapgyn4_surgical_actions','lapgyn4_instrument_count','lapgyn4_anatomical_actions','sklin2_skin_lesions','identify_nbi_infframes','laryngeal_tissues','nerthus_bowel_cleansing_quality','stanford_dogs_image_categorization','svhn','caltech101_object_classification','caltech256_object_classification','cifar10_object_classification','cifar100_object_classification','mnist_digit_classification','emnist_digit_classification','hyperkvasir_anatomical-landmarks','hyperkvasir_pathological-findings','hyperkvasir_quality-of-mucosal-views','hyperkvasir_therapeutic-interventions','cholec80_grasper_presence','cholec80_bipolar_presence','cholec80_hook_presence','cholec80_scissors_presence','cholec80_clipper_presence','cholec80_irrigator_presence','cholec80_specimenbag_presence','derm7pt_skin_lesions','idle_action_recognition','barretts_esophagus_diagnosis','brain_tumor_classification','mednode_melanoma_classification','brain_tumor_type_classification','chexpert_enlarged_cardiomediastinum','chexpert_cardiomegaly','chexpert_lung_opacity','chexpert_lung_lesion','chexpert_edema','chexpert_consolidation','chexpert_pneumonia','chexpert_atelectasis','chexpert_pneumothorax','chexpert_pleural_effusion','chexpert_pleural_other','chexpert_fracture','chexpert_support_devices','pneumonia_classification','ph2-melanocytic-lesions-classification','covid_xray_classification','isic20_melanoma_classification','deep_drid_dr_level','shenzen_chest_xray_tuberculosis','crawled_covid_ct_classification','deep_drid_quality','deep_drid_clarity','deep_drid_field','deep_drid_artifact','kvasir_capsule_anatomy','kvasir_capsule_content','kvasir_capsule_pathologies','breast_cancer_classification_v2','eye_condition_classification','mura_xr_wrist','mura_xr_shoulder','mura_xr_humerus','mura_xr_hand','mura_xr_forearm','mura_xr_finger','mura_xr_elbow','bean_plant_disease_classification','aptos19_blindness_detection','lapgyn4_anatomical_structures+shrink_train?800','lapgyn4_surgical_actions+shrink_train?800','lapgyn4_instrument_count+shrink_train?800','lapgyn4_anatomical_actions+shrink_train?800','laryngeal_tissues+shrink_train?800','nerthus_bowel_cleansing_quality+shrink_train?800','svhn+shrink_train?800','cifar10_object_classification+shrink_train?800','mnist_digit_classification+shrink_train?800','hyperkvasir_anatomical-landmarks+shrink_train?800','hyperkvasir_pathological-findings+shrink_train?800','hyperkvasir_quality-of-mucosal-views+shrink_train?800','hyperkvasir_therapeutic-interventions+shrink_train?800','cholec80_grasper_presence+shrink_train?800','cholec80_bipolar_presence+shrink_train?800','cholec80_hook_presence+shrink_train?800','cholec80_scissors_presence+shrink_train?800','cholec80_clipper_presence+shrink_train?800','cholec80_irrigator_presence+shrink_train?800','cholec80_specimenbag_presence+shrink_train?800','idle_action_recognition+shrink_train?800','brain_tumor_classification+shrink_train?800','brain_tumor_type_classification+shrink_train?800','chexpert_enlarged_cardiomediastinum+shrink_train?800','chexpert_cardiomegaly+shrink_train?800','chexpert_lung_opacity+shrink_train?800','chexpert_lung_lesion+shrink_train?800','chexpert_edema+shrink_train?800','chexpert_consolidation+shrink_train?800','chexpert_pneumonia+shrink_train?800','chexpert_atelectasis+shrink_train?800','chexpert_pneumothorax+shrink_train?800','chexpert_pleural_effusion+shrink_train?800','chexpert_pleural_other+shrink_train?800','chexpert_fracture+shrink_train?800','chexpert_support_devices+shrink_train?800','pneumonia_classification+shrink_train?800','covid_xray_classification+shrink_train?800','isic20_melanoma_classification+shrink_train?800','deep_drid_dr_level+shrink_train?800','deep_drid_quality+shrink_train?800','deep_drid_clarity+shrink_train?800','deep_drid_field+shrink_train?800','deep_drid_artifact+shrink_train?800','kvasir_capsule_anatomy+shrink_train?800','kvasir_capsule_content+shrink_train?800','kvasir_capsule_pathologies+shrink_train?800','mura_xr_wrist+shrink_train?800','mura_xr_shoulder+shrink_train?800','mura_xr_humerus+shrink_train?800','mura_xr_hand+shrink_train?800','mura_xr_forearm+shrink_train?800','mura_xr_finger+shrink_train?800','mura_xr_elbow+shrink_train?800','bean_plant_disease_classification+shrink_train?800','aptos19_blindness_detection+shrink_train?800'] proj=pami2_fims_recent mode.subroutines=['tune','fim'] sampling.sample_num=8000 sampling.balanced=True mode.fim.samples=2000 augmentations=baseline256\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fim_cmd.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " my_source = 'aptos19_blindness_detection'\n",
    " my_target = 'breast_cancer_classification_v2'\n",
    " some_other_task = 'bean_plant_disease_classification'\n",
    " assert my_source in get_valid_sources(my_target)\n",
    " assert some_other_task in shrinkable_tasks\n",
    " from mml_tf.tasks import shrink_map, old_to_new\n",
    " my_shrunk_source = old_to_new(shrink_map[my_source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 12:56:20,418\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Started MML 1.0.2 on Python 3.10.16 with mode CREATE.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:20,419\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Plugins loaded: ['mml-sql', 'mml-similarity', 'mml-dimensionality', 'mml-tasks', 'mml-tf', 'mml-lsf']\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:20,629\u001B[0m][\u001B[34mmml.core.scripts.schedulers.create_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Skipping creation of task bean_plant_disease_classification because there already seems to be a RAW version of that.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:20,629\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Executing after init hook: check_lsf_workers\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:20,629\u001B[0m][\u001B[34mmml_lsf.workers\u001B[0m][\u001B[32mINFO\u001B[0m] - LSF cluster plugin detected local system, no changes made to the number of workers.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:20,630\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML init time was 0.0h 0.0m  0.21s.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:20,632\u001B[0m][\u001B[34mmml.core.scripts.schedulers.create_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting task creation!\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:20,632\u001B[0m][\u001B[34mmml.core.data_loading.file_manager\u001B[0m][\u001B[32mINFO\u001B[0m] - A total of 0 paths have been created during this run.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:20,632\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Successfully finished all experiments!\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:20,633\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML run time was 0.0h 0.0m  0.00s.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    " create_cmd = prep_cmds[0]                                      # pick the task creation job for all tasks\n",
    " create_cmd.config_options['tasks'] = 'none'                    # remove the creation of all tasks\n",
    " create_cmd.config_options['task_list'] = [some_other_task]      # set the single task to be created\n",
    " runner.run(create_cmd)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 12:56:26,407\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Started MML 1.0.2 on Python 3.10.16 with mode PP.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,407\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Plugins loaded: ['mml-sql', 'mml-similarity', 'mml-dimensionality', 'mml-tasks', 'mml-tf', 'mml-lsf']\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,626\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Executing after init hook: check_lsf_workers\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,627\u001B[0m][\u001B[34mmml_lsf.workers\u001B[0m][\u001B[32mINFO\u001B[0m] - LSF cluster plugin detected local system, no changes made to the number of workers.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,629\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML init time was 0.0h 0.0m  0.22s.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,631\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Preparing experiment ...\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,632\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting experiment!\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,632\u001B[0m][\u001B[34mmml.core.scripts.schedulers.preprocess_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting preprocessing data for task \u001B[33m\u001B[46m\u001B[1mbean_plant_disease_classification\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,633\u001B[0m][\u001B[34mmml.core.scripts.schedulers.preprocess_scheduler\u001B[0m][\u001B[33mWARNING\u001B[0m] - Task bean_plant_disease_classification is already preprocessed with default, will skip computations. Delete /home/scholzpa/Pictures/datasets/mml_data/PREPROCESSED/default/DSET_ibean/TASK_bean_plant_disease_classification.json if you want to redo the calculations.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,633\u001B[0m][\u001B[34mmml.core.data_loading.file_manager\u001B[0m][\u001B[32mINFO\u001B[0m] - A total of 0 paths have been created during this run.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,633\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Successfully finished all experiments!\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:26,634\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML run time was 0.0h 0.0m  0.00s.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    " pp_cmd = prep_cmds[1]                                      # pick the task preprocessing job for all tasks\n",
    " pp_cmd.config_options['tasks'] = 'none'                    # remove the preprocessing of all tasks\n",
    " pp_cmd.config_options['task_list'] = [some_other_task]      # set the single task to be preprocessed\n",
    " runner.run(pp_cmd)                                         # run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 12:56:32,452\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Started MML 1.0.2 on Python 3.10.16 with mode INFO.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:32,452\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Plugins loaded: ['mml-sql', 'mml-similarity', 'mml-dimensionality', 'mml-tasks', 'mml-tf', 'mml-lsf']\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:32,665\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Was given no study name to search for, so showing all studies with project prefix.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:32,666\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Executing after init hook: check_lsf_workers\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:32,666\u001B[0m][\u001B[34mmml_lsf.workers\u001B[0m][\u001B[32mINFO\u001B[0m] - LSF cluster plugin detected local system, no changes made to the number of workers.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:32,668\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML init time was 0.0h 0.0m  0.22s.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:32,670\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Preparing experiment ...\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:32,670\u001B[0m][\u001B[34mmml.core.data_loading.task_struct\u001B[0m][\u001B[32mINFO\u001B[0m] - Generating description of bean_plant_disease_classification+shrink_train?800 for preprocessing default.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:32,679\u001B[0m][\u001B[34mmml.core.data_loading.file_manager\u001B[0m][\u001B[32mINFO\u001B[0m] - Writing task description at /home/scholzpa/Pictures/datasets/mml_data/PREPROCESSED/default/DSET_ibean/temp.json.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:32,722\u001B[0m][\u001B[34mmml.core.data_preparation.utils\u001B[0m][\u001B[32mINFO\u001B[0m] - Calculating mean, std and size. This may take a couple of minutes.\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering sizes: 100%|██████████| 1034/1034 [00:00<00:00, 17535.77it/s]\n",
      "Gathering mean and std: 100%|██████████| 11/11 [00:02<00:00,  4.49it/s]\n",
      "Loading samples: 100%|██████████| 1/1 [00:00<00:00, 56.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 12:56:35,442\u001B[0m][\u001B[34mmml.core.data_loading.file_manager\u001B[0m][\u001B[32mINFO\u001B[0m] - Writing task description at /home/scholzpa/Pictures/datasets/mml_data/PREPROCESSED/default/DSET_ibean/TASK_bean_plant_disease_classification+shrink_train?800.json.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,444\u001B[0m][\u001B[34mmml.core.data_preparation.task_creator\u001B[0m][\u001B[32mINFO\u001B[0m] - Testing the loading of /home/scholzpa/Pictures/datasets/mml_data/PREPROCESSED/default/DSET_ibean/TASK_bean_plant_disease_classification+shrink_train?800.json...\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,453\u001B[0m][\u001B[34mmml.core.data_preparation.task_creator\u001B[0m][\u001B[32mINFO\u001B[0m] - Testing of /home/scholzpa/Pictures/datasets/mml_data/PREPROCESSED/default/DSET_ibean/TASK_bean_plant_disease_classification+shrink_train?800.json finished, dataset loading time was  0.01 seconds, sample loading time was  0.00 seconds.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,454\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting experiment!\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,455\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting info on task \u001B[33m\u001B[46m\u001B[1mbean_plant_disease_classification+shrink_train?800\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,455\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Task name: bean_plant_disease_classification+shrink_train?800\n",
      "Task type: classification\n",
      "Num classes: 3\n",
      "Means: RGBInfo(r=0.384707510471344, g=0.41163575649261475, b=0.2485131025314331)\n",
      "Stds: RGBInfo(r=0.26506105065345764, g=0.2833297848701477, b=0.21092694997787476)\n",
      "Sizes: Sizes(min_height=256, max_height=256, min_width=256, max_width=256)\n",
      "Class occ: {'angular_leaf_spot': 274, 'healthy': 261, 'bean_rust': 265}\n",
      "Preprocessed: default\n",
      "Task keywords: ['natural_objects']\n",
      "paths: {}\n",
      "models: []\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,462\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Num samples (full train set): 1034\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,462\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Default validation class occurrences are: {0: 77, 1: 79, 2: 78}\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,463\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished info on task \u001B[33m\u001B[46m\u001B[1mbean_plant_disease_classification+shrink_train?800\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,463\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting plotting sample grid of all tasks.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,465\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/Documents/development/github/mml/src/mml/core/data_loading/lightning_datamodule.py:309: Deactivated normalization for task bean_plant_disease_classification+shrink_train?800.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,495\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished plotting sample grid. Can be found at /home/scholzpa/Documents/exp/tf_repro/pami2_base_02/PLOTS/sample_grid/grid_0002.png.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,506\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished plotting individual samples for each task.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,507\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Model info shows only loaded models (not all existing!). Use reuse.models=... to select a project to load models from.\u001B[0m\n",
      "+------+---------+------+-------------+-----------------+---------+--------+\n",
      "| task | created | fold | performance | training (secs) | params? | preds? |\n",
      "+------+---------+------+-------------+-----------------+---------+--------+\n",
      "+------+---------+------+-------------+-----------------+---------+--------+\n",
      "[\u001B[36m2025-03-14 12:56:35,509\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - No models found for 1 tasks (['bean_plant_disease_classification+shrink_train?800']).\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,509\u001B[0m][\u001B[34mmml.core.scripts.schedulers.info_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Total number of all samples: 1034.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,510\u001B[0m][\u001B[34mmml.core.data_loading.file_manager\u001B[0m][\u001B[32mINFO\u001B[0m] - A total of 2 paths have been created during this run.\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,510\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Successfully finished all experiments!\u001B[0m\n",
      "[\u001B[36m2025-03-14 12:56:35,510\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML run time was 0.0h 0.0m  2.84s.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    " shrink_cmd = prep_cmds[2]                                      # pick the task shrinking job for all tasks\n",
    " shrink_cmd.config_options['tasks'] = 'none'                    # remove the shrinking of all tasks\n",
    " shrink_cmd.config_options['task_list'] = [old_to_new(shrink_map[some_other_task])]      # set the single task to be shrinked\n",
    " runner.run(shrink_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 13:06:49,613\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Started MML 1.0.2 on Python 3.10.16 with mode DIM.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:49,613\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Plugins loaded: ['mml-sql', 'mml-similarity', 'mml-dimensionality', 'mml-tasks', 'mml-tf', 'mml-lsf']\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:49,695\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/Documents/development/github/mml/src/mml/core/scripts/decorators.py:47: UserWarning: Dimension estimation is in beta! The API and functionality of DimensionalityScheduler.__init__ may change without warning in future releases.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:49,910\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Executing after init hook: check_lsf_workers\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:49,910\u001B[0m][\u001B[34mmml_lsf.workers\u001B[0m][\u001B[32mINFO\u001B[0m] - LSF cluster plugin detected local system, no changes made to the number of workers.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:49,912\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML init time was 0.0h 0.0m  0.30s.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:49,914\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Preparing experiment ...\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:49,914\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting experiment!\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:49,915\u001B[0m][\u001B[34mmml_dimensionality.scripts.dimensionality_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting estimating dimensionality data for task \u001B[33m\u001B[46m\u001B[1mbean_plant_disease_classification\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:49,916\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/Documents/development/github/mml/src/mml/core/data_loading/lightning_datamodule.py:309: Deactivated normalization for task bean_plant_disease_classification.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:49,934\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/miniconda3/envs/tf-repro/lib/python3.10/site-packages/mml_dimensionality/scripts/mle.py:45: UserWarning: Subset only has 933 samples!\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate subsets:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Compute KNN:   0%|          | 0/10 [00:00<?, ?it/s]\u001B[A\n",
      "Compute KNN:  10%|█         | 1/10 [00:01<00:11,  1.26s/it]\u001B[A\n",
      "Compute KNN:  20%|██        | 2/10 [00:02<00:08,  1.01s/it]\u001B[A\n",
      "Compute KNN:  30%|███       | 3/10 [00:02<00:06,  1.08it/s]\u001B[A\n",
      "Compute KNN:  40%|████      | 4/10 [00:03<00:05,  1.14it/s]\u001B[A\n",
      "Compute KNN:  50%|█████     | 5/10 [00:04<00:04,  1.17it/s]\u001B[A\n",
      "Compute KNN:  60%|██████    | 6/10 [00:05<00:03,  1.18it/s]\u001B[A\n",
      "Compute KNN:  70%|███████   | 7/10 [00:06<00:02,  1.19it/s]\u001B[A\n",
      "Compute KNN:  80%|████████  | 8/10 [00:07<00:01,  1.19it/s]\u001B[A\n",
      "Compute KNN:  90%|█████████ | 9/10 [00:07<00:00,  1.20it/s]\u001B[A\n",
      "Compute KNN: 100%|██████████| 10/10 [00:08<00:00,  1.20it/s]\u001B[A\n",
      "Iterate subsets: 100%|██████████| 1/1 [00:08<00:00,  8.93s/it]A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 13:06:58,866\u001B[0m][\u001B[34mmml_dimensionality.scripts.dimensionality_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished dimensionality estimation for task \u001B[33m\u001B[46m\u001B[1mbean_plant_disease_classification\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:58,869\u001B[0m][\u001B[34mmml.core.data_loading.file_manager\u001B[0m][\u001B[32mINFO\u001B[0m] - A total of 1 paths have been created during this run.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:58,869\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Successfully finished all experiments!\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:06:58,869\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML run time was 0.0h 0.0m  8.96s.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    " dim_cmd = dim_cmds[0]                                      # pick the dimensionality computing job for all tasks\n",
    " dim_cmd.config_options['tasks'] = 'none'                   # remove the computation of all tasks\n",
    " dim_cmd.config_options['task_list'] = [some_other_task]    # set the single task to be computed\n",
    " runner.run(dim_cmd)                                        # run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 13:13:38,996\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Started MML 1.0.2 on Python 3.10.16 with mode TRAIN.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:38,997\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Plugins loaded: ['mml-sql', 'mml-similarity', 'mml-dimensionality', 'mml-tasks', 'mml-tf', 'mml-lsf']\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,210\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Added pivot task breast_cancer_classification_v2 to task_list.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,210\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Pivot task is \u001B[33m\u001B[46m\u001B[1mbreast_cancer_classification_v2\u001B[0m.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,215\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Executing after init hook: check_lsf_workers\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,215\u001B[0m][\u001B[34mmml_lsf.workers\u001B[0m][\u001B[32mINFO\u001B[0m] - LSF cluster plugin detected local system, no changes made to the number of workers.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,217\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML init time was 0.0h 0.0m  0.22s.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,218\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Preparing experiment ...\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,218\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting experiment!\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,218\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting training for task \u001B[33m\u001B[46m\u001B[1mbreast_cancer_classification_v2\u001B[0m and fold 0.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,563\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,738\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,923\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,935\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,935\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,935\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,936\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Caching disabled while tuning.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:13:39,936\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting learning rate optimization.\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 13:13:40,681\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/miniconda3/envs/tf-repro/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 13:13:44,913\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/miniconda3/envs/tf-repro/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr: 100%|██████████| 100/100 [00:46<00:00,  2.14it/s]\n",
      "Learning rate set to 0.006918309709189364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 13:14:28,640\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - `Trainer.fit` stopped: `max_steps=100` reached.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:14:28,642\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Restoring states from the checkpoint path at /home/scholzpa/Documents/exp/tf_repro/pami2_raw_03_0/runs/2025-03-14/13-13-38-945239/.lr_find_b070f235-71a1-4bf2-ad9e-5bf946f9c505.ckpt\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:14:28,787\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Restored all states from the checkpoint at /home/scholzpa/Documents/exp/tf_repro/pami2_raw_03_0/runs/2025-03-14/13-13-38-945239/.lr_find_b070f235-71a1-4bf2-ad9e-5bf946f9c505.ckpt\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 624/624 [00:00<00:00, 691.76it/s] \n",
      "Caching:   0%|          | 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 13:14:30,002\u001B[0m][\u001B[34mmml.core.data_loading.task_dataset\u001B[0m][\u001B[32mINFO\u001B[0m] - Caching activated for breast_cancer_classification_v2.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:14:30,003\u001B[0m][\u001B[34mmml.core.data_loading.task_dataset\u001B[0m][\u001B[32mINFO\u001B[0m] - Cached 624 samples.\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 156/156 [00:00<00:00, 334.99it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model         | TimmGenericModel | 21.3 M | train\n",
      "1 | criteria      | ModuleDict       | 0      | train\n",
      "2 | train_metrics | ModuleDict       | 0      | train\n",
      "3 | val_metrics   | ModuleDict       | 0      | train\n",
      "4 | test_metrics  | ModuleDict       | 0      | train\n",
      "5 | train_cms     | ModuleDict       | 0      | train\n",
      "6 | val_cms       | ModuleDict       | 0      | train\n",
      "7 | test_cms      | ModuleDict       | 0      | train\n",
      "-----------------------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.3 M    Total params\n",
      "85.145    Total estimated model params size (MB)\n",
      "374       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2025-03-14 13:14:30,469\u001B[0m][\u001B[34mmml.core.data_loading.task_dataset\u001B[0m][\u001B[32mINFO\u001B[0m] - Caching activated for breast_cancer_classification_v2.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:14:30,469\u001B[0m][\u001B[34mmml.core.data_loading.task_dataset\u001B[0m][\u001B[32mINFO\u001B[0m] - Cached 156 samples.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:14:30,523\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Using learning rate 0.006918309709189364.\u001B[0m\n",
      "Epoch 39: 100%|██████████| 3/3 [00:00<00:00,  3.07it/s, train/loss=0.233, exp=2025-03-14/13-13-38-945239] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\u001B[A\n",
      "Epoch 39: 100%|██████████| 3/3 [00:02<00:00,  1.05it/s, train/loss=0.233, exp=2025-03-14/13-13-38-945239][\u001B[36m2025-03-14 13:15:12,111\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - `Trainer.fit` stopped: `max_epochs=40` reached.\u001B[0m\n",
      "Epoch 39: 100%|██████████| 3/3 [00:02<00:00,  1.04it/s, train/loss=0.233, exp=2025-03-14/13-13-38-945239]\n",
      "[\u001B[36m2025-03-14 13:15:12,853\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished training for task \u001B[33m\u001B[46m\u001B[1mbreast_cancer_classification_v2\u001B[0m and fold 0.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,853\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Will try to aggregate validation results over 1 training runs.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,854\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Aggregated validation results over training:\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,854\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - val/breast_cancer_classification_v2/loss : 0.47 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,854\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - val/breast_cancer_classification_v2/MulticlassAccuracy : 0.80 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,854\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - val/breast_cancer_classification_v2/MulticlassF1Score : 0.81 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,854\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - val/breast_cancer_classification_v2/MulticlassAveragePrecision : 0.88 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,855\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - val/breast_cancer_classification_v2/MulticlassAUROC : 0.94 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,855\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - val/breast_cancer_classification_v2/MulticlassMatthewsCorrCoef : 0.71 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,855\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - val/breast_cancer_classification_v2/MulticlassPrecision : 0.84 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,855\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - val/breast_cancer_classification_v2/MulticlassRecall : 0.80 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,855\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - val/breast_cancer_classification_v2/MulticlassCalibrationError : 0.09 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,856\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - train/breast_cancer_classification_v2/loss : 0.23 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,856\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - train/loss : 0.23 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,856\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - train/breast_cancer_classification_v2/MulticlassAccuracy : 0.94 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,856\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - train/breast_cancer_classification_v2/MulticlassF1Score : 0.93 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,856\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - train/breast_cancer_classification_v2/MulticlassAveragePrecision : 0.98 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,856\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - train/breast_cancer_classification_v2/MulticlassAUROC : 0.99 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,857\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - train/breast_cancer_classification_v2/MulticlassMatthewsCorrCoef : 0.88 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,857\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - train/breast_cancer_classification_v2/MulticlassPrecision : 0.93 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,857\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - train/breast_cancer_classification_v2/MulticlassRecall : 0.94 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,857\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - train/breast_cancer_classification_v2/MulticlassCalibrationError : 0.11 ± 0.00\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,857\u001B[0m][\u001B[34mmml.core.data_loading.file_manager\u001B[0m][\u001B[32mINFO\u001B[0m] - A total of 3 paths have been created during this run.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,928\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Successfully finished all experiments!\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,928\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML run time was 0.0h 1.0m 33.71s.\u001B[0m\n",
      "[\u001B[36m2025-03-14 13:15:12,929\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Return value is 0.46628537774086.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    " base_cmd = [cmd for cmd in base_cmds if cmd.config_options['pivot.name'] == my_target][0]  # pick the baseline computing job for the target task\n",
    " base_cmd.config_options['tasks'] = 'none'                   # remove the loading of all tasks\n",
    " runner.run(base_cmd)                                         # run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mml_tf.tasks import test_tasks\n",
    "my_target in test_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not override 'arch.timm.name'.\n",
      "To append to your config use +arch.timm.name=regnetx_032\n",
      "Key 'timm' is not in struct\n",
      "    full_key: arch.timm\n",
      "    object_type=dict\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    }
   ],
   "source": [
    " import random\n",
    " arch = random.choice(list(model_transfer_arch_reqs.keys()))  # either use a random architecture or pick one from `model_transfer_arch_reqs`\n",
    " arch_cmd = [cmd for cmd in arch_cmds if (cmd.config_options['pivot.name'] == my_target and cmd.config_options['arch.timm.name'] == arch)][0]  # pick the first arch computing job for the target task and selected architecture\n",
    " arch_cmd.config_options['tasks'] = 'none'                   # remove the loading of all tasks\n",
    " runner.run(arch_cmd)                                         # run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mml train tasks=none pivot.name=breast_cancer_classification_v2 mode.cv=False mode.nested=False mode.store_parameters=False sampling.balanced=True sampling.batch_size=300 callbacks=none lr_scheduler=step +trainer.check_val_every_n_epoch=40 trainer.max_epochs=40 augmentations=baseline256 sampling.enable_caching=True proj=pami2_t_arch_search_02_0 arch.timm.name=regnetx_032 seed=0'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch_cmd.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-repro",
   "language": "python",
   "name": "tf-repro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
