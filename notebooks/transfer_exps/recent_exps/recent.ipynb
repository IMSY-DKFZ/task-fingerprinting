{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:43:59.899478Z",
     "start_time": "2024-10-07T13:43:34.476682Z"
    }
   },
   "source": [
    "# this notebook generates all commands for the recent mml version \n",
    "import dataclasses\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Union\n",
    "\n",
    "try:\n",
    "    import mml.interactive\n",
    "except ImportError:\n",
    "    raise RuntimeError('This reproduction expects a recent version of MML - please refer to the README for detailed instructions.')\n",
    "\n",
    "mml.interactive.init()\n",
    "from mml.interactive import DefaultRequirements, MMLJobDescription\n",
    "from mml_tf.tasks import all_tasks, get_valid_sources, shrinkable_tasks, target_tasks, source_tasks, train_tasks, \\\n",
    "    all_tasks_including_shrunk\n",
    "\n",
    "CLUSTER_USAGE = False  # change if you (want / do not want) to run on the cluster\n",
    "\n",
    "if CLUSTER_USAGE:\n",
    "    from mml_lsf.requirements import LSFSubmissionRequirements"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scholzpa/Documents/development/gitlab/mml/src/mml/interactive/__init__.py:49: UserWarning: Did not provide a \"env_path\", neither found set \"MML_ENV_PATH\" variable, you might need to provide \"env_path\" to \"init\" in order to use \"mml\" interactively in a jupyter/ipython setting.\n",
      "  warnings.warn(\n",
      "/home/scholzpa/miniconda3/envs/mml/lib/python3.8/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _____ ______   _____ ______   ___\n",
      "|\\   _ \\  _   \\|\\   _ \\  _   \\|\\  \\\n",
      "\\ \\  \\\\\\__\\ \\  \\ \\  \\\\\\__\\ \\  \\ \\  \\\n",
      " \\ \\  \\\\|__| \\  \\ \\  \\\\|__| \\  \\ \\  \\\n",
      "  \\ \\  \\    \\ \\  \\ \\  \\    \\ \\  \\ \\  \\____\n",
      "   \\ \\__\\    \\ \\__\\ \\__\\    \\ \\__\\ \\_______\\\n",
      "    \\|__|     \\|__|\\|__|     \\|__|\\|_______|\n",
      "         ____  _  _    __  _  _  ____  _  _\n",
      "        (  _ \\( \\/ )  (  )( \\/ )/ ___)( \\/ )\n",
      "         ) _ ( )  /    )( / \\/ \\\\___ \\ )  /\n",
      "        (____/(__/    (__)\\_)(_/(____/(__/\n",
      "Interactive MML API initialized.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "task_infos = mml.interactive.get_task_infos(task_list=all_tasks, dims=None)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:44:31.584301Z",
     "start_time": "2024-10-07T13:44:10.401534Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# convenience function for easier retrieve from cluster results\n",
    "user_id = 'USERNAME'\n",
    "# note that final experiments have to be run multiple times to ensure significance\n",
    "rerun = 3\n",
    "\n",
    "\n",
    "# use as print(get_retrieve_for_proj('my_project')) and run the result in a shell to get the results of 'my_project' from cluster to your local system\n",
    "def get_retrieve_for_proj(proj):\n",
    "    return f\"rsync -rtvu --stats --exclude=PARAMETERS --exclude=hpo --exclude=runs --exclude=FIMS --exclude=FC_TUNED {user_id}@odcf-worker01:{os.getenv('MML_CLUSTER_RESULTS_PATH')}/{proj}/ {os.getenv('MML_RESULTS_PATH')}/{proj}\"\n",
    "\n",
    "\n",
    "# the following optimizes a jobs epochs in a way that target task is seen at least 40 epochs but at max 4000 steps (plus finishing the epoch)\n",
    "def optimize_epochs(target_task: str, batch_size: int = 300, max_steps: int = 4000, max_epoch: int = 40) -> int:\n",
    "    return min(max_epoch, (max_steps // ((int(task_infos.num_samples[target_task] * 0.8) // batch_size) + 1)) + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:44:41.891784Z",
     "start_time": "2024-10-07T13:44:41.887337Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "if CLUSTER_USAGE:\n",
    "    # cluster submission prepends, add yours here in case you have other gpu requirements\n",
    "    base_reqs = LSFSubmissionRequirements(special_requirements=[],\n",
    "                                          undesired_hosts=['e230-dgx2-2', 'e230-dgxa100-2', 'e230-dgxa100-4',\n",
    "                                                           'e230-dgxa100-1',\n",
    "                                                           'e230-dgxa100-2', 'e230-dgxa100-3', 'e230-dgxa100-4',\n",
    "                                                           'lsf22-gpu08', 'lsf22-gpu01', 'lsf22-gpu02', 'lsf22-gpu03',\n",
    "                                                           'lsf22-gpu04', 'lsf22-gpu05', 'lsf22-gpu06', 'lsf22-gpu07'],\n",
    "                                          num_gpus=1, vram_per_gpu=11.0, queue='gpu-lowprio',\n",
    "                                          mail='EMAIL.ADDRESS@dkfz-heidelberg.de', script_name='mml.sh',\n",
    "                                          job_group='/USERNAME/pami_rerun'\n",
    "                                          )\n",
    "    # see for example this local setup\n",
    "    # base_reqs = pp_reqs = aa_reqs = def_reqs = arch_reqs = tl_reqs = multi_reqs = nb.DefaultRequirements()\n",
    "    pp_reqs = dataclasses.replace(base_reqs, queue='gpu')\n",
    "    aa_reqs = dataclasses.replace(base_reqs, script_name='aa.sh', vram_per_gpu=13.0)\n",
    "    def_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'])\n",
    "    tl_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'], vram_per_gpu=24.0)\n",
    "    multi_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'], vram_per_gpu=14.0)\n",
    "else:\n",
    "    base_reqs = pp_reqs = aa_reqs = def_reqs = tl_reqs = multi_reqs = DefaultRequirements()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:44:43.595248Z",
     "start_time": "2024-10-07T13:44:43.591147Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# project overview -> points to MML projects we use\n",
    "projects = {\n",
    "    'base': 'pami2_base_02',\n",
    "    'fed_hpo': 'pami2_fed_hpo_10',\n",
    "    'raw_baseline': 'pami2_raw_03',\n",
    "    'raw_shrunk': 'pami2_raw_shrunk_10',\n",
    "    # aa search was misplaced by accident, this is resolved within exp 3\n",
    "    'aa_search': 'pami2_t_aa_search_01',\n",
    "    # the above are shared with train (!) since stuff is only computed once anyway\n",
    "    'pretrain': 'pami2_t_pretrain_01',\n",
    "    'transfer': 'pami2_t_transfer_20',\n",
    "    'multi_task': 'test_multi_balanced_test_split_10',  # 'pami2_t_multi_task_02',\n",
    "    'multi_shrunk': 'test_multi_balanced_shrunk_test_split_10',  # 'pami2_t_multi_shrunk_02',\n",
    "    'arch_search': 'pami2_t_arch_search_02',\n",
    "    'arch_infer': 'pami2_t_arch_infer_02',\n",
    "    'aa_infer': 'pami2_t_aa_infer_02'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:44:44.825643Z",
     "start_time": "2024-10-07T13:44:44.821801Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare steps\n",
    "prep_cmds = list()\n",
    "# step one: plain task creation\n",
    "\n",
    "prep_cmds.append(MMLJobDescription(prefix_req=pp_reqs, mode='create',\n",
    "                                   config_options={'tasks': 'pami', 'proj': projects['base']}))\n",
    "# step two: plain task preprocessing\n",
    "prep_cmds.append(MMLJobDescription(prefix_req=pp_reqs, mode='pp',\n",
    "                                   config_options={'tasks': 'pami', 'proj': projects['base']}))\n",
    "# step three: shrunk preprocessing\n",
    "prep_cmds.append(MMLJobDescription(prefix_req=pp_reqs, mode='info',\n",
    "                                   config_options={'tasks': 'pami_shrinkable_800', 'proj': projects['base']}))\n",
    "mml.interactive.write_out_commands(cmd_list=prep_cmds, name='prepare')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:44:45.688755Z",
     "start_time": "2024-10-07T13:44:45.682433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 3 commands at prepare.txt.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "# OPTIONALLY: compute dimensions (used for Fig. 3) and some additional experiments not shown in the paper\n",
    "dim_cmds = list()\n",
    "dim_cmds.append(MMLJobDescription(prefix_req=def_reqs, mode='dim', config_options={'tasks': 'pami_shrink_mix',\n",
    "                                                                                   'proj': projects[\"base\"],\n",
    "                                                                                   'mode.inv_mle': True}))\n",
    "mml.interactive.write_out_commands(cmd_list=dim_cmds, name='dimensions')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:44:47.014032Z",
     "start_time": "2024-10-07T13:44:47.007565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 1 commands at dimensions.txt.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# baselines\n",
    "# these are the default options for all tasks, they should not be modified without justification\n",
    "def get_default_config(target_task: str, shrunk: bool = False) -> Dict[str, Union[str, bool, int]]:\n",
    "    if shrunk:\n",
    "        epochs = 40\n",
    "    else:\n",
    "        epochs = optimize_epochs(target_task=target_task, batch_size=300, max_steps=4000, max_epoch=40)\n",
    "    default_options = {'tasks': 'pami', 'pivot.name': t, 'mode.cv': False, 'mode.nested': False,\n",
    "                       'mode.store_parameters': False, 'sampling.balanced': True,\n",
    "                       'sampling.batch_size': 300, 'callbacks': 'none', 'lr_scheduler': 'step',\n",
    "                       '+trainer.check_val_every_n_epoch': epochs,\n",
    "                       'trainer.max_epochs': epochs, 'augmentations': 'baseline256', 'sampling.enable_caching': True}\n",
    "    return default_options\n",
    "\n",
    "\n",
    "base_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in all_tasks:\n",
    "        opts = get_default_config(t)\n",
    "        opts.update({'proj': f'{projects[\"raw_baseline\"]}_{ix}', 'seed': ix, 'mode.store_parameters': True})\n",
    "        base_cmds.append(MMLJobDescription(prefix_req=def_reqs, mode='train', config_options=opts))\n",
    "        if t in shrinkable_tasks:\n",
    "            shrink_opts = get_default_config(t, shrunk=True)\n",
    "            shrink_opts.update({'proj': f'{projects[\"raw_shrunk\"]}_{ix}', 'tasks': 'pami_shrink'})\n",
    "            base_cmds.append(MMLJobDescription(prefix_req=def_reqs, mode='train', config_options=shrink_opts))\n",
    "mml.interactive.write_out_commands(cmd_list=base_cmds, name='baseline')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:44:48.066009Z",
     "start_time": "2024-10-07T13:44:48.054802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 381 commands at baseline.txt.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "#################################\n",
    "# EXPERIMENT 1: MODEL TRANSFER  #\n",
    "#################################\n",
    "# VRAM requirements for timm architectures\n",
    "model_transfer_arch_reqs = {\n",
    "    'tf_efficientnet_b2': 23.0,\n",
    "    'tf_efficientnet_b2_ap': 24.0,\n",
    "    'tf_efficientnet_b2_ns': 24.0,\n",
    "    'tf_efficientnet_cc_b0_4e': 22.0,\n",
    "    'swsl_resnet50': 20.0,\n",
    "    'ssl_resnext50_32x4d': 24.0,\n",
    "    'regnetx_032': 20.5,\n",
    "    'regnety_032': 22.0,\n",
    "    'rexnet_100': 20.5,\n",
    "    'ecaresnet50d': 24.0,\n",
    "    'cspdarknet53': 23.0,\n",
    "    'mixnet_l': 25.0,\n",
    "    'cspresnext50': 24.0,\n",
    "    'cspresnet50': 18.0,\n",
    "    'ese_vovnet39b': 25.0,\n",
    "    'resnest50d': 25.5,\n",
    "    'hrnet_w18': 24.0,\n",
    "    'skresnet34': 16.5,\n",
    "    'mobilenetv3_large_100': 13.5,\n",
    "    'res2net50_26w_4s': 24.5\n",
    "}\n",
    "arch_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in source_tasks:\n",
    "        for arch, vram in model_transfer_arch_reqs.items():\n",
    "            opts = get_default_config(t)\n",
    "            opts.update({'proj': f'{projects[\"arch_search\"]}_{ix}',\n",
    "                         'arch.timm.name': arch, 'seed': ix})\n",
    "            # the following goes back to a rare occurrence of incompatible singleton batches with some batch_norms\n",
    "            # avoid this by minimally wiggle batch size\n",
    "            if t == 'mura_xr_wrist' and arch in ['rexnet_100', 'resnest50d', 'skresnet34']:\n",
    "                opts.update({'sampling.batch_size': 301})\n",
    "            if CLUSTER_USAGE:\n",
    "                arch_reqs = dataclasses.replace(def_reqs, vram_per_gpu=vram)\n",
    "            else:\n",
    "                arch_reqs = def_reqs\n",
    "            arch_cmds.append(MMLJobDescription(prefix_req=arch_reqs, mode='train',\n",
    "                                               config_options=opts))\n",
    "mml.interactive.write_out_commands(cmd_list=arch_cmds, name='full_arch', max_cmds=2000)\n",
    "arch_shrunk_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        if task_infos.num_classes[t] > 40 or task_infos.num_samples[t] <= 1000:\n",
    "            continue\n",
    "        for arch, vram in model_transfer_arch_reqs.items():\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update({'proj': f'{projects[\"arch_infer\"]}_{ix}', 'tasks': 'pami_shrink',\n",
    "                         'arch.classification.id': arch, 'seed': ix})\n",
    "            if CLUSTER_USAGE:\n",
    "                arch_reqs = dataclasses.replace(def_reqs, vram_per_gpu=vram)\n",
    "            else:\n",
    "                arch_reqs = def_reqs\n",
    "            arch_shrunk_cmds.append(MMLJobDescription(prefix_req=arch_reqs, mode='train',\n",
    "                                                         config_options=opts))\n",
    "mml.interactive.write_out_commands(cmd_list=arch_shrunk_cmds, name='arch_shrunk', max_cmds=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:49:34.771241Z",
     "start_time": "2024-10-07T13:49:34.648496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at full_arch_0.txt.\n",
      "Stored 2000 commands at full_arch_1.txt.\n",
      "Stored 260 commands at full_arch_2.txt.\n",
      "Stored 2000 commands at arch_shrunk_0.txt.\n",
      "Stored 160 commands at arch_shrunk_1.txt.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "####################################\n",
    "# EXPERIMENT 2: TRANSFER LEARNING  #\n",
    "####################################\n",
    "trans_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # only small tasks are used as targets\n",
    "        if task_infos.num_classes[t] > 40:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            mod_task_file = 'pami' if task_infos.num_samples[t] <= 1000 else 'pami_shrink'\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update({'proj': f'{projects[\"transfer\"]}_{ix}', 'tasks': mod_task_file,\n",
    "                         'reuse.models': f'{projects[\"raw_baseline\"]}_{ix}', 'mode.pretrain_task': s,\n",
    "                         'seed': ix})\n",
    "            trans_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='tl'))\n",
    "mml.interactive.write_out_commands(cmd_list=trans_cmds, name='transfer', max_cmds=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:49:41.425832Z",
     "start_time": "2024-10-07T13:49:41.230564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at transfer_0.txt.\n",
      "Stored 2000 commands at transfer_1.txt.\n",
      "Stored 2000 commands at transfer_2.txt.\n",
      "Stored 2000 commands at transfer_3.txt.\n",
      "Stored 496 commands at transfer_4.txt.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "######################################\n",
    "# EXPERIMENT 3: AUG POLICY TRANSFER  #\n",
    "######################################\n",
    "# Step 1:  training the auto augmentation pipeline for each potential source\n",
    "warnings.warn(\"AA mode is not supported anymore with the recent version of MML, you need to import the following projects manually -> pami2_t_aa_search_01 and pami2_aa_search_01\")\n",
    "# Step 2: evaluating the augmentation pipeline\n",
    "policy_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # only small tasks are used as targets\n",
    "        if task_infos.num_classes[t] > 40:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            mod_task_file = 'pami' if task_infos.num_samples[t] <= 1000 else 'pami_shrink'\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            # resolving an accident from above\n",
    "            reuse_aa_proj = f'pami2_aa_search_01_{ix}' if s in train_tasks else f'pami2_t_aa_search_01_{ix}'\n",
    "            # and another accident\n",
    "            if 'breast' in s:\n",
    "                reuse_aa_proj = f'pami2_t_aa_search_01_{ix}'\n",
    "            opts.update({'proj': f'{projects[\"aa_infer\"]}_{ix}', 'tasks': mod_task_file,\n",
    "                         'reuse.aa': reuse_aa_proj, 'augmentations': 'load_aa_from',\n",
    "                         'augmentations.source': s, 'seed': ix})\n",
    "            policy_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='train'))\n",
    "mml.interactive.write_out_commands(cmd_list=policy_cmds, name='policy', max_cmds=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:49:42.971977Z",
     "start_time": "2024-10-07T13:49:42.758062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at policy_0.txt.\n",
      "Stored 2000 commands at policy_1.txt.\n",
      "Stored 2000 commands at policy_2.txt.\n",
      "Stored 2000 commands at policy_3.txt.\n",
      "Stored 496 commands at policy_4.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3261094/1949134110.py:5: UserWarning: AA mode is not supported anymore with the recent version of MML, you need to import the following projects manually -> pami2_t_aa_search_01 and pami2_aa_search_01\n",
      "  warnings.warn(\"AA mode is not supported anymore with the recent version of MML, you need to import the following projects manually -> pami2_t_aa_search_01 and pami2_aa_search_01\")\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "######################################\n",
    "# EXPERIMENT 4: MULTI-TASK LEARNING  #\n",
    "######################################\n",
    "# We did not use full multitask learning with full sized target tasks in the paper\n",
    "# multi_cmds = list()\n",
    "# for ix in range(rerun):\n",
    "#     for t in target_tasks:\n",
    "#         for s in get_valid_sources(t):\n",
    "#             opts = get_default_config(t)\n",
    "#             opts.update(\n",
    "#                 {\n",
    "#                     'proj': f'{projects[\"multi_task\"]}_{ix}',\n",
    "#                     'mode.multitask': 2,\n",
    "#                     'sampling.balanced': True,\n",
    "#                     'mode.co_tasks': [s],\n",
    "#                     'sampling.sample_num': int(0.8 * task_infos.num_samples[t]),\n",
    "#                     'loss.auto_activate_weighing': False, 'seed': ix})\n",
    "#             multi_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='train'))\n",
    "# mml.interactive.write_out_commands(cmd_list=multi_cmds, name='full_multi', max_cmds=2000)\n",
    "\n",
    "multi_shrunk_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # unshrinkable or already covered above\n",
    "        if task_infos.num_classes[t] > 40 or task_infos.num_samples[t] <= 1000:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update(\n",
    "                {'tasks': 'pami_shrink',\n",
    "                 'proj': f'{projects[\"multi_shrunk\"]}_{ix}',\n",
    "                 'mode.multitask': 2,\n",
    "                 'sampling.balanced': True,\n",
    "                 'mode.co_tasks': [s],\n",
    "                 'sampling.sample_num': min(int(0.8 * task_infos.num_samples[t]), 800),\n",
    "                 'loss.auto_activate_weighing': False, 'seed': ix})\n",
    "            multi_shrunk_cmds.append(MMLJobDescription(prefix_req=def_reqs, config_options=opts, mode='train'))\n",
    "mml.interactive.write_out_commands(cmd_list=multi_shrunk_cmds, name='multi_shrunk', max_cmds=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:51:54.738208Z",
     "start_time": "2024-10-07T13:51:54.496434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2000 commands at multi_shrunk_0.txt.\n",
      "Stored 2000 commands at multi_shrunk_1.txt.\n",
      "Stored 2000 commands at multi_shrunk_2.txt.\n",
      "Stored 1026 commands at multi_shrunk_3.txt.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "all_train_cmds = base_cmds + arch_cmds + arch_shrunk_cmds + trans_cmds + policy_cmds + multi_shrunk_cmds\n",
    "print(f'Our experiments trained {len(all_train_cmds)} models.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:51:55.544274Z",
     "start_time": "2024-10-07T13:51:55.536534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our experiments trained 30819 models.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "# if you want to submit jobs to the cluster or run them locally, consider the runner functionality\n",
    "# see mml_lsf README instructions on how to set this up \n",
    "# the following demonstrates submission of the baseline jobs\n",
    "if CLUSTER_USAGE:\n",
    "    from mml_lsf.runner import LSFJobRunner\n",
    "\n",
    "    runner = LSFJobRunner()\n",
    "    for job in base_cmds:\n",
    "        runner.run(job)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:52:04.821166Z",
     "start_time": "2024-10-07T13:52:04.818227Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "sync_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for proj_id in ['multi_task', 'aa_infer', 'transfer', 'arch_search', 'raw_shrunk',\n",
    "                    'raw_baseline', 'multi_shrunk', 'arch_infer']:\n",
    "        sync_cmds.append(get_retrieve_for_proj(f'{projects[proj_id]}_{ix}'))\n",
    "# the following was only used by accident\n",
    "with open(Path(os.path.abspath('')) / 'output_sync.txt', 'w') as file:\n",
    "    file.write('\\n'.join(sync_cmds))\n",
    "print(f'Stored {len(sync_cmds)} commands at output_sync.txt.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:52:20.859397Z",
     "start_time": "2024-10-07T13:52:20.853470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 24 commands at output_sync.txt.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature and FIM extraction\n",
    "\n",
    "This is how task feature extraction works. Note that full features comprise several GB and are not provided directly (also for licensing compatibility issues). The computed task distances are provided in the `cache` folder top-level."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "updated_shrunk_task_list = [t.replace(' --shrink_train 800', '+shrink_train?800') for t in all_tasks_including_shrunk]\n",
    "\n",
    "features_cmd = MMLJobDescription(prefix_req=def_reqs,\n",
    "                                 config_options={'task_list': updated_shrunk_task_list, 'proj': 'pami2_features',\n",
    "                                                 'mode.subroutines': ['feature'], 'augmentations': 'baseline256'},\n",
    "                                 mode='emd')\n",
    "fim_cmd = MMLJobDescription(prefix_req=def_reqs,\n",
    "                            config_options={'task_list': updated_shrunk_task_list, 'proj': 'pami2_fims_recent',\n",
    "                                            'mode.subroutines': ['tune', 'fim'], 'sampling.sample_num': 8000,\n",
    "                                            'sampling.balanced': True, 'mode.fim.samples': 2000,\n",
    "                                            'augmentations': 'baseline256', }, mode='fed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:52:27.489479Z",
     "start_time": "2024-10-07T13:52:27.484478Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "# the following demonstrates how to run these locally from within this notebook\n",
    "# CAUTION: it produces a lot of logging output to the notebook - consider running these commands in the terminal as described below\n",
    "from mml.interactive import SubprocessJobRunner\n",
    "\n",
    "local_reqs = DefaultRequirements()\n",
    "runner = SubprocessJobRunner()\n",
    "for job in [features_cmd, fim_cmd]:\n",
    "    job.prefix_req = local_reqs\n",
    "    # runner.run(job)  # uncomment to run"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# want to run in the terminal - follow here\n",
    "local_reqs = DefaultRequirements()\n",
    "for job in [features_cmd, fim_cmd]:\n",
    "    job.prefix_req = local_reqs\n",
    "features_cmd.render()  # paste the output into terminal (remove surrounding quotes) takes ~20 minutes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:53:21.417395Z",
     "start_time": "2024-10-07T13:53:21.410812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mml emd task_list=['lapgyn4_anatomical_structures','lapgyn4_surgical_actions','lapgyn4_instrument_count','lapgyn4_anatomical_actions','sklin2_skin_lesions','identify_nbi_infframes','laryngeal_tissues','nerthus_bowel_cleansing_quality','stanford_dogs_image_categorization','svhn','caltech101_object_classification','caltech256_object_classification','cifar10_object_classification','cifar100_object_classification','mnist_digit_classification','emnist_digit_classification','hyperkvasir_anatomical-landmarks','hyperkvasir_pathological-findings','hyperkvasir_quality-of-mucosal-views','hyperkvasir_therapeutic-interventions','cholec80_grasper_presence','cholec80_bipolar_presence','cholec80_hook_presence','cholec80_scissors_presence','cholec80_clipper_presence','cholec80_irrigator_presence','cholec80_specimenbag_presence','derm7pt_skin_lesions','idle_action_recognition','barretts_esophagus_diagnosis','brain_tumor_classification','mednode_melanoma_classification','brain_tumor_type_classification','chexpert_enlarged_cardiomediastinum','chexpert_cardiomegaly','chexpert_lung_opacity','chexpert_lung_lesion','chexpert_edema','chexpert_consolidation','chexpert_pneumonia','chexpert_atelectasis','chexpert_pneumothorax','chexpert_pleural_effusion','chexpert_pleural_other','chexpert_fracture','chexpert_support_devices','pneumonia_classification','ph2-melanocytic-lesions-classification','covid_xray_classification','isic20_melanoma_classification','deep_drid_dr_level','shenzen_chest_xray_tuberculosis','crawled_covid_ct_classification','deep_drid_quality','deep_drid_clarity','deep_drid_field','deep_drid_artifact','kvasir_capsule_anatomy','kvasir_capsule_content','kvasir_capsule_pathologies','breast_cancer_classification_v2','eye_condition_classification','mura_xr_wrist','mura_xr_shoulder','mura_xr_humerus','mura_xr_hand','mura_xr_forearm','mura_xr_finger','mura_xr_elbow','bean_plant_disease_classification','aptos19_blindness_detection','lapgyn4_anatomical_structures+shrink_train?800','lapgyn4_surgical_actions+shrink_train?800','lapgyn4_instrument_count+shrink_train?800','lapgyn4_anatomical_actions+shrink_train?800','laryngeal_tissues+shrink_train?800','nerthus_bowel_cleansing_quality+shrink_train?800','svhn+shrink_train?800','cifar10_object_classification+shrink_train?800','mnist_digit_classification+shrink_train?800','hyperkvasir_anatomical-landmarks+shrink_train?800','hyperkvasir_pathological-findings+shrink_train?800','hyperkvasir_quality-of-mucosal-views+shrink_train?800','hyperkvasir_therapeutic-interventions+shrink_train?800','cholec80_grasper_presence+shrink_train?800','cholec80_bipolar_presence+shrink_train?800','cholec80_hook_presence+shrink_train?800','cholec80_scissors_presence+shrink_train?800','cholec80_clipper_presence+shrink_train?800','cholec80_irrigator_presence+shrink_train?800','cholec80_specimenbag_presence+shrink_train?800','idle_action_recognition+shrink_train?800','brain_tumor_classification+shrink_train?800','brain_tumor_type_classification+shrink_train?800','chexpert_enlarged_cardiomediastinum+shrink_train?800','chexpert_cardiomegaly+shrink_train?800','chexpert_lung_opacity+shrink_train?800','chexpert_lung_lesion+shrink_train?800','chexpert_edema+shrink_train?800','chexpert_consolidation+shrink_train?800','chexpert_pneumonia+shrink_train?800','chexpert_atelectasis+shrink_train?800','chexpert_pneumothorax+shrink_train?800','chexpert_pleural_effusion+shrink_train?800','chexpert_pleural_other+shrink_train?800','chexpert_fracture+shrink_train?800','chexpert_support_devices+shrink_train?800','pneumonia_classification+shrink_train?800','covid_xray_classification+shrink_train?800','isic20_melanoma_classification+shrink_train?800','deep_drid_dr_level+shrink_train?800','deep_drid_quality+shrink_train?800','deep_drid_clarity+shrink_train?800','deep_drid_field+shrink_train?800','deep_drid_artifact+shrink_train?800','kvasir_capsule_anatomy+shrink_train?800','kvasir_capsule_content+shrink_train?800','kvasir_capsule_pathologies+shrink_train?800','mura_xr_wrist+shrink_train?800','mura_xr_shoulder+shrink_train?800','mura_xr_humerus+shrink_train?800','mura_xr_hand+shrink_train?800','mura_xr_forearm+shrink_train?800','mura_xr_finger+shrink_train?800','mura_xr_elbow+shrink_train?800','bean_plant_disease_classification+shrink_train?800','aptos19_blindness_detection+shrink_train?800'] proj=pami2_features mode.subroutines=['feature'] augmentations=baseline256\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "fim_cmd.render()",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T13:53:22.830871Z",
     "start_time": "2024-10-07T13:53:22.826546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mml fed task_list=['lapgyn4_anatomical_structures','lapgyn4_surgical_actions','lapgyn4_instrument_count','lapgyn4_anatomical_actions','sklin2_skin_lesions','identify_nbi_infframes','laryngeal_tissues','nerthus_bowel_cleansing_quality','stanford_dogs_image_categorization','svhn','caltech101_object_classification','caltech256_object_classification','cifar10_object_classification','cifar100_object_classification','mnist_digit_classification','emnist_digit_classification','hyperkvasir_anatomical-landmarks','hyperkvasir_pathological-findings','hyperkvasir_quality-of-mucosal-views','hyperkvasir_therapeutic-interventions','cholec80_grasper_presence','cholec80_bipolar_presence','cholec80_hook_presence','cholec80_scissors_presence','cholec80_clipper_presence','cholec80_irrigator_presence','cholec80_specimenbag_presence','derm7pt_skin_lesions','idle_action_recognition','barretts_esophagus_diagnosis','brain_tumor_classification','mednode_melanoma_classification','brain_tumor_type_classification','chexpert_enlarged_cardiomediastinum','chexpert_cardiomegaly','chexpert_lung_opacity','chexpert_lung_lesion','chexpert_edema','chexpert_consolidation','chexpert_pneumonia','chexpert_atelectasis','chexpert_pneumothorax','chexpert_pleural_effusion','chexpert_pleural_other','chexpert_fracture','chexpert_support_devices','pneumonia_classification','ph2-melanocytic-lesions-classification','covid_xray_classification','isic20_melanoma_classification','deep_drid_dr_level','shenzen_chest_xray_tuberculosis','crawled_covid_ct_classification','deep_drid_quality','deep_drid_clarity','deep_drid_field','deep_drid_artifact','kvasir_capsule_anatomy','kvasir_capsule_content','kvasir_capsule_pathologies','breast_cancer_classification_v2','eye_condition_classification','mura_xr_wrist','mura_xr_shoulder','mura_xr_humerus','mura_xr_hand','mura_xr_forearm','mura_xr_finger','mura_xr_elbow','bean_plant_disease_classification','aptos19_blindness_detection','lapgyn4_anatomical_structures+shrink_train?800','lapgyn4_surgical_actions+shrink_train?800','lapgyn4_instrument_count+shrink_train?800','lapgyn4_anatomical_actions+shrink_train?800','laryngeal_tissues+shrink_train?800','nerthus_bowel_cleansing_quality+shrink_train?800','svhn+shrink_train?800','cifar10_object_classification+shrink_train?800','mnist_digit_classification+shrink_train?800','hyperkvasir_anatomical-landmarks+shrink_train?800','hyperkvasir_pathological-findings+shrink_train?800','hyperkvasir_quality-of-mucosal-views+shrink_train?800','hyperkvasir_therapeutic-interventions+shrink_train?800','cholec80_grasper_presence+shrink_train?800','cholec80_bipolar_presence+shrink_train?800','cholec80_hook_presence+shrink_train?800','cholec80_scissors_presence+shrink_train?800','cholec80_clipper_presence+shrink_train?800','cholec80_irrigator_presence+shrink_train?800','cholec80_specimenbag_presence+shrink_train?800','idle_action_recognition+shrink_train?800','brain_tumor_classification+shrink_train?800','brain_tumor_type_classification+shrink_train?800','chexpert_enlarged_cardiomediastinum+shrink_train?800','chexpert_cardiomegaly+shrink_train?800','chexpert_lung_opacity+shrink_train?800','chexpert_lung_lesion+shrink_train?800','chexpert_edema+shrink_train?800','chexpert_consolidation+shrink_train?800','chexpert_pneumonia+shrink_train?800','chexpert_atelectasis+shrink_train?800','chexpert_pneumothorax+shrink_train?800','chexpert_pleural_effusion+shrink_train?800','chexpert_pleural_other+shrink_train?800','chexpert_fracture+shrink_train?800','chexpert_support_devices+shrink_train?800','pneumonia_classification+shrink_train?800','covid_xray_classification+shrink_train?800','isic20_melanoma_classification+shrink_train?800','deep_drid_dr_level+shrink_train?800','deep_drid_quality+shrink_train?800','deep_drid_clarity+shrink_train?800','deep_drid_field+shrink_train?800','deep_drid_artifact+shrink_train?800','kvasir_capsule_anatomy+shrink_train?800','kvasir_capsule_content+shrink_train?800','kvasir_capsule_pathologies+shrink_train?800','mura_xr_wrist+shrink_train?800','mura_xr_shoulder+shrink_train?800','mura_xr_humerus+shrink_train?800','mura_xr_hand+shrink_train?800','mura_xr_forearm+shrink_train?800','mura_xr_finger+shrink_train?800','mura_xr_elbow+shrink_train?800','bean_plant_disease_classification+shrink_train?800','aptos19_blindness_detection+shrink_train?800'] proj=pami2_fims_recent mode.subroutines=['tune','fim'] sampling.sample_num=8000 sampling.balanced=True mode.fim.samples=2000 augmentations=baseline256\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
