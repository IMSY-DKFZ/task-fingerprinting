{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.402702Z",
     "end_time": "2023-04-25T10:05:50.480885Z"
    }
   },
   "source": [
    "# Generates MML commands for the task fingerprinting knowledge transfer experiments (original version)\n",
    "# This notebook generates the `output_XXX.txt` files next to it, that may be submitted to the DKFZ cluster to run experiments\n",
    "\n",
    "import dataclasses\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Union\n",
    "try:\n",
    "    import mml.api.notebooks as nb\n",
    "except ImportError:\n",
    "    raise RuntimeError('For the original reproduction you need to install an older version of mml, please refer to the README for precise instructions.')\n",
    "nb.init()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# avoid to install mml_tf and provide some data from mml_tf.tasks\n",
    "train_tasks = (\n",
    "    'lapgyn4_anatomical_structures', 'lapgyn4_surgical_actions', 'lapgyn4_instrument_count',\n",
    "    'lapgyn4_anatomical_actions',\n",
    "    'sklin2_skin_lesions', 'identify_nbi_infframes', 'laryngeal_tissues', 'nerthus_bowel_cleansing_quality',\n",
    "    'stanford_dogs_image_categorization', 'svhn', 'caltech101_object_classification',\n",
    "    'caltech256_object_classification',\n",
    "    'cifar10_object_classification', 'cifar100_object_classification', 'mnist_digit_classification',\n",
    "    'emnist_digit_classification', 'hyperkvasir_anatomical-landmarks', 'hyperkvasir_pathological-findings',\n",
    "    'hyperkvasir_quality-of-mucosal-views', 'hyperkvasir_therapeutic-interventions', 'cholec80_grasper_presence',\n",
    "    'cholec80_bipolar_presence', 'cholec80_hook_presence', 'cholec80_scissors_presence', 'cholec80_clipper_presence',\n",
    "    'cholec80_irrigator_presence', 'cholec80_specimenbag_presence', 'derm7pt_skin_lesions')\n",
    "test_tasks = ('idle_action_recognition', 'barretts_esophagus_diagnosis', 'brain_tumor_classification',\n",
    "              'mednode_melanoma_classification', 'brain_tumor_type_classification',\n",
    "              'chexpert_enlarged_cardiomediastinum', 'chexpert_cardiomegaly', 'chexpert_lung_opacity',\n",
    "              'chexpert_lung_lesion', 'chexpert_edema', 'chexpert_consolidation', 'chexpert_pneumonia',\n",
    "              'chexpert_atelectasis', 'chexpert_pneumothorax', 'chexpert_pleural_effusion', 'chexpert_pleural_other',\n",
    "              'chexpert_fracture', 'chexpert_support_devices', \n",
    "              'covid-19-chest-ct-image-augmentation_raw', # this task was originally contained, but might be omitted\n",
    "              'pneumonia_classification', 'ph2-melanocytic-lesions-classification',\n",
    "              'covid_xray_classification', 'isic20_melanoma_classification', 'deep_drid_dr_level',\n",
    "              'shenzen_chest_xray_tuberculosis', 'crawled_covid_ct_classification', 'deep_drid_quality',\n",
    "              'deep_drid_clarity', 'deep_drid_field', 'deep_drid_artifact', 'kvasir_capsule_anatomy',\n",
    "              'kvasir_capsule_content', 'kvasir_capsule_pathologies', 'breast_cancer_classification_v2',\n",
    "              'eye_condition_classification', 'mura_xr_wrist', 'mura_xr_shoulder', 'mura_xr_humerus', 'mura_xr_hand',\n",
    "              'mura_xr_forearm', 'mura_xr_finger', 'mura_xr_elbow', 'bean_plant_disease_classification',\n",
    "              'aptos19_blindness_detection')\n",
    "all_tasks = train_tasks + test_tasks\n",
    "task_infos = nb.get_task_infos(task_list=all_tasks, dims=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.445848Z",
     "end_time": "2023-04-25T10:05:50.567764Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# convenience function for easier retrieve from cluster results\n",
    "user_id = 'USERNAME'  # replace with your AD account\n",
    "\n",
    "# use as print(get_retrieve_for_proj('my_project')) and run the result in a shell to get the results of 'my_project' from cluster to your local system\n",
    "def get_retrieve_for_proj(proj):\n",
    "    return f\"rsync -rtvu --stats --exclude=PARAMETERS --exclude=hpo --exclude=runs --exclude=FIMS --exclude=FC_TUNED {user_id}@odcf-worker01:{os.getenv('MML_CLUSTER_RESULTS_PATH')}/{proj}/ {os.getenv('MML_RESULTS_PATH')}/{proj}\"\n",
    "\n",
    "# the following optimizes a jobs epochs in a way that target task is seen at least 40 epochs but at max 4000 steps (plus finishing the epoch)\n",
    "def optimize_epochs(target_task: str, batch_size: int = 300, max_steps: int = 4000, max_epoch: int = 40) -> int:\n",
    "    return min(max_epoch, (max_steps // ((int(task_infos.num_samples[target_task] * 0.8) // batch_size) + 1)) + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T10:48:22.463021Z",
     "start_time": "2024-06-25T10:48:22.459538Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# cluster submission prepends, add yours here in case you have other gpu requirements\n",
    "base_reqs = nb.LSFSubmissionRequirements(special_requirements=[],\n",
    "                                         undesired_hosts=['e230-dgx2-2', 'e230-dgxa100-2', 'e230-dgxa100-4',\n",
    "                                                          'e230-dgxa100-1',\n",
    "                                                          'e230-dgxa100-2', 'e230-dgxa100-3', 'e230-dgxa100-4', 'lsf22-gpu08', 'lsf22-gpu01', 'lsf22-gpu02', 'lsf22-gpu03', 'lsf22-gpu04', 'lsf22-gpu05', 'lsf22-gpu06', 'lsf22-gpu07'],\n",
    "                                         num_gpus=1, vram_per_gpu=11.0, queue='gpu-lowprio',\n",
    "                                         mail='EMAIL.ADDRESS@dkfz-heidelberg.de', script_name='mml.sh',\n",
    "                                         job_group='/USERNAME/pami_rerun'\n",
    "                                         )\n",
    "# alternatively you may use this local setup\n",
    "# base_reqs = pp_reqs = aa_reqs = def_reqs = arch_reqs = tl_reqs = multi_reqs = nb.DefaultRequirements()\n",
    "pp_reqs = dataclasses.replace(base_reqs, queue='gpu')\n",
    "aa_reqs = dataclasses.replace(base_reqs, script_name='aa.sh', vram_per_gpu=13.0)\n",
    "def_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'])\n",
    "tl_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'], vram_per_gpu=24.0)\n",
    "multi_reqs = dataclasses.replace(base_reqs, special_requirements=['tensorcore'], vram_per_gpu=14.0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.597405Z",
     "end_time": "2023-04-25T10:05:50.620377Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# project overview -> points to MML projects we use\n",
    "projects_train = {\n",
    "    'base': 'pami2_base_02',\n",
    "    'fed_hpo': 'pami2_fed_hpo_03',\n",
    "    'dist_results': 'pami2_dist_02',\n",
    "    'new_fed': 'pami2_new_fed_03',\n",
    "    'raw_baseline': 'pami2_raw_03',\n",
    "    'pretrain': 'pami2_pretrain_10',\n",
    "    'raw_shrunk': 'pami2_raw_shrunk_10',\n",
    "    'transfer': 'pami2_transfer_20',\n",
    "    'multi_task': 'test_multi_balanced_10',\n",
    "    'multi_shrunk': 'test_multi_balanced_shrunk_10',\n",
    "    'arch_search': 'pami2_arch_search_02',\n",
    "    'arch_infer': 'pami2_arch_infer_02',\n",
    "    'aa_search': 'pami2_aa_search_01',\n",
    "    'aa_infer': 'pami2_aa_infer_02'\n",
    "}\n",
    "projects_test = {\n",
    "    'base': 'pami2_base_02',\n",
    "    'fed_hpo': 'pami2_fed_hpo_03',\n",
    "    'dist_results': 'pami2_dist_02',\n",
    "    'new_fed': 'pami2_new_fed_03',\n",
    "    'raw_baseline': 'pami2_raw_03',\n",
    "    'raw_shrunk': 'pami2_raw_shrunk_10',\n",
    "    # aa search was misplaced by accident, this is resolved within exp 3\n",
    "    'aa_search': 'pami2_t_aa_search_01',\n",
    "    # the above are shared with train (!) since stuff is only computed once anyway\n",
    "    'pretrain': 'pami2_t_pretrain_01',\n",
    "    'transfer': 'pami2_t_transfer_20',\n",
    "    'multi_task': 'test_multi_balanced_test_split_10',\n",
    "    'multi_shrunk': 'test_multi_balanced_shrunk_test_split_10',\n",
    "    'arch_search': 'pami2_t_arch_search_02',\n",
    "    'arch_infer': 'pami2_t_arch_infer_02',\n",
    "    'aa_infer': 'pami2_t_aa_infer_02'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.597602Z",
     "end_time": "2023-04-25T10:05:50.620454Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# we distinguish the training period (setting of all hyperparemeters) and later evaluation on the test tasks\n",
    "phase = 'train'  # set to 'test' later on\n",
    "# note that experiments have to be run multiple times to ensure significance\n",
    "rerun = {'train': 3, 'test': 3}[phase]\n",
    "projects = {'train': projects_train, 'test': projects_test}[phase]\n",
    "target_tasks = {'train': train_tasks, 'test': test_tasks}[phase]  # these will become the targets\n",
    "source_tasks = {'train': train_tasks, 'test': train_tasks + test_tasks}[phase]  # these will become the possible sources\n",
    "task_file = {'train': 'pami_train', 'test': 'pami'}[phase]  # see mml_tf/configs/tasks "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.597780Z",
     "end_time": "2023-04-25T10:05:50.620522Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# tasks sharing images should not be inspected together\n",
    "task_groups = {\n",
    "    'chexpert': [\n",
    "        'chexpert_cardiomegaly',\n",
    "        'chexpert_lung_opacity',\n",
    "        'chexpert_lung_lesion',\n",
    "        'chexpert_edema',\n",
    "        'chexpert_consolidation',\n",
    "        'chexpert_pneumonia',\n",
    "        'chexpert_atelectasis',\n",
    "        'chexpert_pneumothorax',\n",
    "        'chexpert_pleural_effusion',\n",
    "        'chexpert_pleural_other',\n",
    "        'chexpert_fracture',\n",
    "        'chexpert_support_devices',\n",
    "    ],\n",
    "    'cholec80': [\n",
    "        'cholec80_grasper_presence',\n",
    "        'cholec80_bipolar_presence',\n",
    "        'cholec80_hook_presence',\n",
    "        'cholec80_scissors_presence',\n",
    "        'cholec80_clipper_presence',\n",
    "        'cholec80_irrigator_presence',\n",
    "        'cholec80_specimenbag_presence',\n",
    "        'lapgyn4_instrument_count'\n",
    "    ],\n",
    "    'deep_drid': [\n",
    "        'deep_drid_dr_level',\n",
    "        'deep_drid_quality',\n",
    "        'deep_drid_clarity',\n",
    "        'deep_drid_field',\n",
    "        'deep_drid_artifact'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# returns valid source tasks respecting phase and task_groups\n",
    "def get_valid_sources(target_task):\n",
    "    # check if target is in a group\n",
    "    group_id = None\n",
    "    for group_name, group_list in task_groups.items():\n",
    "        if target_task in group_list:\n",
    "            group_id = group_name\n",
    "            break\n",
    "    # if so further reduce valid sources\n",
    "    if group_id:\n",
    "        return [t for t in source_tasks if t not in task_groups[group_id]]\n",
    "    return [t for t in source_tasks if t != target_task]\n",
    "\n",
    "\n",
    "def shrinkable(task_name):\n",
    "    # the task shrinking operation only makes sense if there are certain restrictions met\n",
    "    return task_infos.num_classes[task_name] < 40 and task_infos.num_samples[task_name] > 1000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.597944Z",
     "end_time": "2023-04-25T10:05:50.620589Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# prepare steps\n",
    "prep_cmds = list()\n",
    "# step one: plain task creation\n",
    "\n",
    "prep_cmds.append(nb.MMLJobDescription(prefix_req=pp_reqs,\n",
    "                                      config_options={'mode': 'create', 'tasks': 'pami', 'proj': projects['base']}))\n",
    "# step two: plain task preprocessing\n",
    "# either sequentially (best option!)\n",
    "prep_cmds.append(nb.MMLJobDescription(prefix_req=pp_reqs,\n",
    "                                      config_options={'mode': 'pp', 'tasks': 'pami', 'proj': projects['base']}))\n",
    "# or parallel (this causes problems for some tasks!)\n",
    "# for t in all_tasks:\n",
    "#     prep_cmds.append(nb.MMLJobDescription(prefix_req=pp_reqs,\n",
    "#                                           config_options={'mode': 'pp', 'tasks': 'pami',\n",
    "#                                                           'proj': projects['base'], 'pivot.name': t}))\n",
    "# step three: shrinking preprocessed tasks\n",
    "prep_cmds.append(nb.MMLJobDescription(prefix_req=pp_reqs,\n",
    "                                      config_options={'mode': 'info', 'tasks': 'pami_shrinkable_800', 'proj': projects['base']}))\n",
    "nb.write_out_commands(cmd_list=prep_cmds, suffix='prep')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.598231Z",
     "end_time": "2023-04-25T10:05:50.620752Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Submit output_prep.txt commands to set up everything. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# OPTIONALLY: compute dimensions (used for Fig. 3) and some additional experiments not shown in the paper\n",
    "dim_cmds = list()\n",
    "# will do so locally, so use loc_reqs\n",
    "loc_reqs = nb.DefaultRequirements()\n",
    "dim_cmds.append(nb.MMLJobDescription(prefix_req=loc_reqs, config_options={'mode': 'dim', 'tasks': 'pami_shrink_mix',\n",
    "                                                                           'proj': projects[\"base\"],\n",
    "                                                                           'mode.inv_mle': True}))\n",
    "nb.write_out_commands(cmd_list=dim_cmds, suffix='dim')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.598455Z",
     "end_time": "2023-04-25T10:05:50.620879Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Submit output_dim.txt commands to compute dimensions. ",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# baselines\n",
    "# these are the default options for all tasks, they should not be modified without justification\n",
    "def get_default_config(target_task: str, shrunk: bool = False) -> Dict[str, Union[str, bool, int]]:\n",
    "    if shrunk:\n",
    "        epochs = 40\n",
    "    else:\n",
    "        epochs = optimize_epochs(target_task=target_task, batch_size=300, max_steps=4000, max_epoch=40)\n",
    "    default_options = {'mode': 'opt', 'tasks': task_file, 'pivot.name': t,\n",
    "                       'mode.store_parameters': False, 'sampling.balanced': True,\n",
    "                       'sampling.batch_size': 300, 'callbacks': 'none', 'lr_scheduler': 'step',\n",
    "                       'trainer.max_epochs': epochs, 'augmentations': 'baseline256', 'sampling.enable_caching': True}\n",
    "    # during test we don't need to validate every epoch\n",
    "    if phase != 'train':\n",
    "        default_options.update({'+trainer.check_val_every_n_epoch': epochs})\n",
    "    return default_options\n",
    "\n",
    "\n",
    "base_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in all_tasks:\n",
    "        opts = get_default_config(t)\n",
    "        opts.update({'proj': f'{projects[\"raw_baseline\"]}_{ix}', 'seed': ix, 'mode.store_parameters': True})\n",
    "        base_cmds.append(nb.MMLJobDescription(prefix_req=def_reqs, config_options=opts))\n",
    "        if shrinkable(t):\n",
    "            shrink_opts = get_default_config(t)\n",
    "            shrink_opts.update(\n",
    "                {'proj': f'{projects[\"raw_shrunk\"]}_{ix}', 'tasks': f'{task_file}_shrink'})\n",
    "            base_cmds.append(nb.MMLJobDescription(prefix_req=def_reqs, config_options=shrink_opts))\n",
    "nb.write_out_commands(cmd_list=base_cmds, suffix='base')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.598718Z",
     "end_time": "2023-04-25T10:05:50.621006Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Submit output_base.txt commands to set up everything.",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "#################################\n",
    "# EXPERIMENT 1: MODEL TRANSFER  #\n",
    "#################################\n",
    "# VRAM requirements for timm architectures\n",
    "model_transfer_arch_reqs = {\n",
    "    'tf_efficientnet_b2': 23.0,\n",
    "    'tf_efficientnet_b2_ap': 24.0,\n",
    "    'tf_efficientnet_b2_ns': 24.0,\n",
    "    'tf_efficientnet_cc_b0_4e': 22.0,\n",
    "    'swsl_resnet50': 20.0,\n",
    "    'ssl_resnext50_32x4d': 24.0,\n",
    "    'regnetx_032': 20.5,\n",
    "    'regnety_032': 22.0,\n",
    "    'rexnet_100': 20.5,\n",
    "    'ecaresnet50d': 24.0,\n",
    "    'cspdarknet53': 23.0,\n",
    "    'mixnet_l': 25.0,\n",
    "    'cspresnext50': 24.0,\n",
    "    'cspresnet50': 18.0,\n",
    "    'ese_vovnet39b': 25.0,\n",
    "    'resnest50d': 25.5,\n",
    "    'hrnet_w18': 24.0,\n",
    "    'skresnet34': 16.5,\n",
    "    'mobilenetv3_large_100': 13.5,\n",
    "    'res2net50_26w_4s': 24.5\n",
    "}\n",
    "arch_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in source_tasks:\n",
    "        for arch, vram in model_transfer_arch_reqs.items():\n",
    "            opts = get_default_config(t)\n",
    "            opts.update({'proj': f'{projects[\"arch_search\"]}_{ix}',\n",
    "                         'arch.classification.id': arch, 'seed': ix})\n",
    "            # the following goes back to a rare occurrence of incompatible singleton batches with some batch_norms\n",
    "            # avoid this by minimally wiggle batch size\n",
    "            if t == 'mura_xr_wrist' and arch in ['rexnet_100', 'resnest50d', 'skresnet34']:\n",
    "                opts.update({'sampling.batch_size': 301})\n",
    "            arch_reqs = dataclasses.replace(def_reqs, vram_per_gpu=vram)\n",
    "            arch_cmds.append(nb.MMLJobDescription(prefix_req=arch_reqs,\n",
    "                                                  config_options=opts))\n",
    "nb.write_out_commands(cmd_list=arch_cmds, suffix='arch', max_cmds=2000)\n",
    "arch_shrunk_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        if task_infos.num_classes[t] > 40:\n",
    "            continue\n",
    "        if task_infos.num_samples[t] <= 1000:\n",
    "            continue\n",
    "        mod_task_file = f'{task_file}' if task_infos.num_samples[t] <= 1000 else f'{task_file}_shrink'\n",
    "        for arch, vram in model_transfer_arch_reqs.items():\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update({'proj': f'{projects[\"arch_infer\"]}_{ix}', 'tasks': mod_task_file,\n",
    "                         'arch.classification.id': arch, 'seed': ix})\n",
    "            arch_reqs = dataclasses.replace(def_reqs, vram_per_gpu=vram)\n",
    "            arch_shrunk_cmds.append(nb.MMLJobDescription(prefix_req=arch_reqs,\n",
    "                                                         config_options=opts))\n",
    "nb.write_out_commands(cmd_list=arch_shrunk_cmds, suffix='arch_shrunk', max_cmds=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.645849Z",
     "end_time": "2023-04-25T10:05:50.780905Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "####################################\n",
    "# EXPERIMENT 2: TRANSFER LEARNING  #\n",
    "####################################\n",
    "trans_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # only small tasks are used as targets\n",
    "        if task_infos.num_classes[t] > 40:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            mod_task_file = f'{task_file}' if task_infos.num_samples[t] <= 1000 else f'{task_file}_shrink'\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update({'proj': f'{projects[\"transfer\"]}_{ix}', 'tasks': mod_task_file,\n",
    "                         'mode': 'tl', 'reuse.models': f'{projects[\"raw_baseline\"]}_{ix}', 'mode.pretrain_task': s,\n",
    "                         'seed': ix})\n",
    "            del opts['mode.store_parameters']\n",
    "            trans_cmds.append(nb.MMLJobDescription(prefix_req=def_reqs, config_options=opts))\n",
    "nb.write_out_commands(cmd_list=trans_cmds, suffix='trans', max_cmds=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.865766Z",
     "end_time": "2023-04-25T10:05:50.910028Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "######################################\n",
    "# EXPERIMENT 3: AUG POLICY TRANSFER  #\n",
    "######################################\n",
    "# Step 1:  training the auto augmentation pipeline for each potential source\n",
    "aa_cmds = list()\n",
    "for s in source_tasks:\n",
    "    for ix in range(rerun):\n",
    "        opts = {'mode': 'aa', 'proj': f'{projects[\"aa_search\"]}_{ix}', 'tasks': task_file, 'pivot.name': s,\n",
    "                'trainer.max_epochs': optimize_epochs(s, batch_size=120, max_epoch=100, max_steps=10000),\n",
    "                'arch.pretrained': True, 'seed': ix}\n",
    "        aa_cmds.append(nb.MMLJobDescription(prefix_req=aa_reqs, config_options=opts))\n",
    "nb.write_out_commands(cmd_list=aa_cmds, suffix='aa')\n",
    "# Step 2: evaluating the augmentation pipeline\n",
    "policy_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # only small tasks are used as targets\n",
    "        if task_infos.num_classes[t] > 40:\n",
    "            continue\n",
    "        for s in get_valid_sources(t):\n",
    "            mod_task_file = f'{task_file}' if task_infos.num_samples[t] <= 1000 else f'{task_file}_shrink'\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            # resolving an accident from above\n",
    "            reuse_aa_proj = f'{projects_train[\"aa_search\"]}_{ix}' if s in train_tasks else f'{projects_test[\"aa_search\"]}_{ix}'\n",
    "            # and another accident\n",
    "            if 'breast' in s:\n",
    "                reuse_aa_proj = f'{projects_train[\"aa_search\"]}_{ix}'\n",
    "            opts.update({'proj': f'{projects[\"aa_infer\"]}_{ix}', 'tasks': mod_task_file,\n",
    "                         'reuse.aa': reuse_aa_proj, 'augmentations': 'load_aa_from',\n",
    "                         'augmentations.source': s, 'seed': ix})\n",
    "            policy_cmds.append(nb.MMLJobDescription(prefix_req=def_reqs, config_options=opts))\n",
    "nb.write_out_commands(cmd_list=policy_cmds, suffix='policy', max_cmds=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:50.909937Z",
     "end_time": "2023-04-25T10:05:51.009125Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "######################################\n",
    "# EXPERIMENT 4: MULTI-TASK LEARNING  #\n",
    "######################################\n",
    "\n",
    "multi_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        for s in get_valid_sources(t):\n",
    "            opts = get_default_config(t)\n",
    "            opts.update(\n",
    "                {\n",
    "                    'proj': f'{projects[\"multi_task\"]}_{ix}',\n",
    "                    'mode': 'multi',\n",
    "                    'sampling.balanced': True,\n",
    "                 'mode.num_tasks': 2, 'mode.possible_tasks': [s], 'sampling.sample_num': int(1.6 * task_infos.num_samples[t]),\n",
    "                 'loss.auto_activate_weighing': False, 'seed': ix})\n",
    "            del opts['mode.store_parameters']\n",
    "            multi_cmds.append(nb.MMLJobDescription(prefix_req=def_reqs, config_options=opts))\n",
    "nb.write_out_commands(cmd_list=multi_cmds, suffix='multi', max_cmds=2000)\n",
    "\n",
    "multi_shrunk_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for t in target_tasks:\n",
    "        # unshrinkable\n",
    "        if task_infos.num_classes[t] > 40:\n",
    "            continue\n",
    "        # already covered above\n",
    "        if task_infos.num_samples[t] <= 1000:\n",
    "            continue\n",
    "        mod_task_file = f'{task_file}' if task_infos.num_samples[t] <= 1000 else f'{task_file}_shrink'\n",
    "        for s in get_valid_sources(t):\n",
    "            opts = get_default_config(t, shrunk=True)\n",
    "            opts.update(\n",
    "                {\n",
    "                    'proj': f'{projects[\"multi_shrunk\"]}_{ix}',\n",
    "                    'mode': 'multi',\n",
    "                    'sampling.balanced': True,\n",
    "                 'mode.num_tasks': 2, 'mode.possible_tasks': [s], 'tasks': mod_task_file,\n",
    "                 'sampling.sample_num': min(int(1.6 * task_infos.num_samples[t]), 1600),\n",
    "                 'loss.auto_activate_weighing': False, 'seed': ix})\n",
    "            del opts['mode.store_parameters']\n",
    "            multi_shrunk_cmds.append(nb.MMLJobDescription(prefix_req=def_reqs, config_options=opts))\n",
    "nb.write_out_commands(cmd_list=multi_shrunk_cmds, suffix='multi_shrunk', max_cmds=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:51.014385Z",
     "end_time": "2023-04-25T10:05:51.252118Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "sync_cmds = list()\n",
    "for ix in range(rerun):\n",
    "    for proj_id in ['multi_task', 'aa_infer', 'transfer', 'arch_search', 'raw_shrunk', 'pretrain',\n",
    "                    'raw_baseline', 'arch_infer', 'multi_shrunk']:\n",
    "        sync_cmds.append(get_retrieve_for_proj(f'{projects[proj_id]}_{ix}'))\n",
    "with open(Path(os.path.abspath('')) / 'output_sync.txt', 'w') as file:\n",
    "    file.write('\\n'.join(sync_cmds))\n",
    "print(f'Stored {len(sync_cmds)} commands at output_sync.txt.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:05:51.252442Z",
     "end_time": "2023-04-25T10:05:51.254821Z"
    }
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
